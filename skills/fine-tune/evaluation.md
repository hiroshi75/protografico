# è©•ä¾¡æ–¹æ³•ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹è©•ä¾¡æˆ¦ç•¥ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€‚

**ğŸ’¡ Tip**: å®Ÿè·µçš„ãªè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ [examples.md](examples.md#phase-2-ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ä¾‹) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ“Š è©•ä¾¡ã®é‡è¦æ€§

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦è©•ä¾¡ã¯ï¼š
- **æ”¹å–„ã®å®šé‡åŒ–**: å®¢è¦³çš„ãªé€²æ—æ¸¬å®š
- **æ„æ€æ±ºå®šã®æ ¹æ‹ **: ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå„ªå…ˆé †ä½ä»˜ã‘
- **å“è³ªä¿è¨¼**: ãƒªã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®é˜²æ­¢
- **ROI ã®è¨¼æ˜**: ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã®å¯è¦–åŒ–

## ğŸ¯ è©•ä¾¡æŒ‡æ¨™ã®è¨­è¨ˆ

### ä¸»è¦ãªè©•ä¾¡æŒ‡æ¨™ã‚«ãƒ†ã‚´ãƒª

#### 1. å“è³ªæŒ‡æ¨™ï¼ˆQuality Metricsï¼‰

**Accuracyï¼ˆæ­£è§£ç‡ï¼‰**
```python
def calculate_accuracy(predictions: List, ground_truth: List) -> float:
    """æ­£è§£ç‡ã‚’è¨ˆç®—"""
    correct = sum(p == g for p, g in zip(predictions, ground_truth))
    return (correct / len(predictions)) * 100

# ä¾‹
predictions = ["product", "technical", "billing", "general"]
ground_truth = ["product", "billing", "billing", "general"]
accuracy = calculate_accuracy(predictions, ground_truth)
# => 50.0% (2/4ãŒæ­£è§£)
```

**F1 Scoreï¼ˆãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰**
```python
from sklearn.metrics import f1_score, classification_report

def calculate_f1(predictions: List, ground_truth: List, average='weighted') -> float:
    """F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å¯¾å¿œï¼‰"""
    return f1_score(ground_truth, predictions, average=average)

# è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
report = classification_report(ground_truth, predictions)
print(report)
"""
              precision    recall  f1-score   support

     product       1.00      1.00      1.00         1
   technical       0.00      0.00      0.00         1
     billing       0.50      1.00      0.67         1
     general       1.00      1.00      1.00         1

    accuracy                           0.75         4
   macro avg       0.62      0.75      0.67         4
weighted avg       0.62      0.75      0.67         4
"""
```

**Semantic Similarityï¼ˆæ„å‘³çš„é¡ä¼¼åº¦ï¼‰**
```python
from sentence_transformers import SentenceTransformer, util

def calculate_semantic_similarity(
    generated: str,
    reference: str,
    model_name: str = "all-MiniLM-L6-v2"
) -> float:
    """ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¨å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³çš„é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
    model = SentenceTransformer(model_name)

    embeddings = model.encode([generated, reference], convert_to_tensor=True)
    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])

    return similarity.item()

# ä¾‹
generated = "Our premium plan costs $49 per month."
reference = "The premium subscription is $49/month."
similarity = calculate_semantic_similarity(generated, reference)
# => 0.87 (é«˜ã„é¡ä¼¼åº¦)
```

**BLEU Scoreï¼ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå“è³ªï¼‰**
```python
from nltk.translate.bleu_score import sentence_bleu

def calculate_bleu(generated: str, reference: str) -> float:
    """BLEU ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    reference_tokens = [reference.split()]
    generated_tokens = generated.split()

    return sentence_bleu(reference_tokens, generated_tokens)

# ä¾‹
generated = "The product costs forty nine dollars"
reference = "The product costs $49"
bleu = calculate_bleu(generated, reference)
# => 0.45
```

#### 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ï¼ˆPerformance Metricsï¼‰

**Latencyï¼ˆå¿œç­”æ™‚é–“ï¼‰**
```python
import time
from typing import Dict, List

def measure_latency(test_cases: List[Dict]) -> Dict:
    """å„ãƒãƒ¼ãƒ‰ã¨ãƒˆãƒ¼ã‚¿ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã‚’æ¸¬å®š"""
    results = {
        "total": [],
        "by_node": {}
    }

    for case in test_cases:
        start_time = time.time()

        # ãƒãƒ¼ãƒ‰ã”ã¨ã®è¨ˆæ¸¬
        node_times = {}

        # Node 1: analyze_intent
        node_start = time.time()
        analyze_result = analyze_intent(case["input"])
        node_times["analyze_intent"] = time.time() - node_start

        # Node 2: retrieve_context
        node_start = time.time()
        context = retrieve_context(analyze_result)
        node_times["retrieve_context"] = time.time() - node_start

        # Node 3: generate_response
        node_start = time.time()
        response = generate_response(context, case["input"])
        node_times["generate_response"] = time.time() - node_start

        total_time = time.time() - start_time

        results["total"].append(total_time)
        for node, duration in node_times.items():
            if node not in results["by_node"]:
                results["by_node"][node] = []
            results["by_node"][node].append(duration)

    # çµ±è¨ˆè¨ˆç®—
    import numpy as np
    summary = {
        "total": {
            "mean": np.mean(results["total"]),
            "p50": np.percentile(results["total"], 50),
            "p95": np.percentile(results["total"], 95),
            "p99": np.percentile(results["total"], 99),
        }
    }

    for node, times in results["by_node"].items():
        summary[node] = {
            "mean": np.mean(times),
            "p50": np.percentile(times, 50),
            "p95": np.percentile(times, 95),
        }

    return summary

# ä½¿ç”¨ä¾‹
latency_results = measure_latency(test_cases)
print(f"Mean latency: {latency_results['total']['mean']:.2f}s")
print(f"P95 latency: {latency_results['total']['p95']:.2f}s")
```

**Throughputï¼ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼‰**
```python
import concurrent.futures
from typing import List, Dict

def measure_throughput(
    test_cases: List[Dict],
    max_workers: int = 10,
    duration_seconds: int = 60
) -> Dict:
    """ä¸€å®šæ™‚é–“å†…ã®å‡¦ç†æ•°ã‚’æ¸¬å®š"""
    start_time = time.time()
    completed = 0
    errors = 0

    def process_case(case):
        try:
            result = run_langgraph_app(case["input"])
            return True
        except Exception:
            return False

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        while time.time() - start_time < duration_seconds:
            # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ãƒ«ãƒ¼ãƒ—
            for case in test_cases:
                if time.time() - start_time >= duration_seconds:
                    break

                future = executor.submit(process_case, case)
                if future.result():
                    completed += 1
                else:
                    errors += 1

    elapsed = time.time() - start_time

    return {
        "completed": completed,
        "errors": errors,
        "elapsed": elapsed,
        "throughput": completed / elapsed,  # requests per second
        "error_rate": errors / (completed + errors) if (completed + errors) > 0 else 0
    }

# ä½¿ç”¨ä¾‹
throughput = measure_throughput(test_cases, max_workers=5, duration_seconds=30)
print(f"Throughput: {throughput['throughput']:.2f} req/s")
print(f"Error rate: {throughput['error_rate']*100:.2f}%")
```

#### 3. ã‚³ã‚¹ãƒˆæŒ‡æ¨™ï¼ˆCost Metricsï¼‰

**Token Usage ã¨ã‚³ã‚¹ãƒˆ**
```python
from typing import Dict

# ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®æ–™é‡‘è¡¨ï¼ˆ2024å¹´11æœˆæ™‚ç‚¹ï¼‰
PRICING = {
    "claude-3-5-sonnet-20241022": {
        "input": 3.0 / 1_000_000,   # $3.00 per 1M input tokens
        "output": 15.0 / 1_000_000,  # $15.00 per 1M output tokens
    },
    "claude-3-5-haiku-20241022": {
        "input": 0.8 / 1_000_000,   # $0.80 per 1M input tokens
        "output": 4.0 / 1_000_000,   # $4.00 per 1M output tokens
    }
}

def calculate_cost(token_usage: Dict, model: str) -> Dict:
    """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‹ã‚‰ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
    pricing = PRICING.get(model, PRICING["claude-3-5-sonnet-20241022"])

    input_cost = token_usage["input_tokens"] * pricing["input"]
    output_cost = token_usage["output_tokens"] * pricing["output"]
    total_cost = input_cost + output_cost

    return {
        "input_tokens": token_usage["input_tokens"],
        "output_tokens": token_usage["output_tokens"],
        "total_tokens": token_usage["input_tokens"] + token_usage["output_tokens"],
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost,
        "cost_breakdown": {
            "input_pct": (input_cost / total_cost * 100) if total_cost > 0 else 0,
            "output_pct": (output_cost / total_cost * 100) if total_cost > 0 else 0
        }
    }

# ä½¿ç”¨ä¾‹
token_usage = {"input_tokens": 1500, "output_tokens": 800}
cost = calculate_cost(token_usage, "claude-3-5-sonnet-20241022")
print(f"Total cost: ${cost['total_cost']:.4f}")
print(f"Input: ${cost['input_cost']:.4f} ({cost['cost_breakdown']['input_pct']:.1f}%)")
print(f"Output: ${cost['output_cost']:.4f} ({cost['cost_breakdown']['output_pct']:.1f}%)")
```

**Cost per Request**
```python
def calculate_cost_per_request(
    test_results: List[Dict],
    model: str
) -> Dict:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
    total_cost = 0
    total_input_tokens = 0
    total_output_tokens = 0

    for result in test_results:
        cost = calculate_cost(result["token_usage"], model)
        total_cost += cost["total_cost"]
        total_input_tokens += result["token_usage"]["input_tokens"]
        total_output_tokens += result["token_usage"]["output_tokens"]

    num_requests = len(test_results)

    return {
        "total_requests": num_requests,
        "total_cost": total_cost,
        "cost_per_request": total_cost / num_requests,
        "avg_input_tokens": total_input_tokens / num_requests,
        "avg_output_tokens": total_output_tokens / num_requests,
        "total_tokens": total_input_tokens + total_output_tokens
    }
```

#### 4. ä¿¡é ¼æ€§æŒ‡æ¨™ï¼ˆReliability Metricsï¼‰

**Error Rateï¼ˆã‚¨ãƒ©ãƒ¼ç‡ï¼‰**
```python
def calculate_error_rate(results: List[Dict]) -> Dict:
    """ã‚¨ãƒ©ãƒ¼ç‡ã¨ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ"""
    total = len(results)
    errors = [r for r in results if r.get("error")]

    error_types = {}
    for error in errors:
        error_type = error["error"]["type"]
        if error_type not in error_types:
            error_types[error_type] = 0
        error_types[error_type] += 1

    return {
        "total_requests": total,
        "total_errors": len(errors),
        "error_rate": len(errors) / total if total > 0 else 0,
        "error_types": error_types,
        "success_rate": (total - len(errors)) / total if total > 0 else 0
    }
```

**Retry Rateï¼ˆãƒªãƒˆãƒ©ã‚¤ç‡ï¼‰**
```python
def calculate_retry_rate(results: List[Dict]) -> Dict:
    """ãƒªãƒˆãƒ©ã‚¤ãŒå¿…è¦ã ã£ãŸã‚±ãƒ¼ã‚¹ã®å‰²åˆ"""
    total = len(results)
    retried = [r for r in results if r.get("retry_count", 0) > 0]

    return {
        "total_requests": total,
        "retried_requests": len(retried),
        "retry_rate": len(retried) / total if total > 0 else 0,
        "avg_retries": sum(r.get("retry_count", 0) for r in retried) / len(retried) if retried else 0
    }
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è¨­è¨ˆ

### ä»£è¡¨çš„ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®æ§‹é€ 

```json
{
  "test_cases": [
    {
      "id": "TC001",
      "category": "product_inquiry",
      "difficulty": "easy",
      "input": "How much does the premium plan cost?",
      "expected_intent": "product_inquiry",
      "expected_answer": "The premium plan costs $49 per month.",
      "expected_answer_semantic": ["premium", "plan", "$49", "month"],
      "metadata": {
        "user_type": "new",
        "context_required": false
      }
    },
    {
      "id": "TC002",
      "category": "technical_support",
      "difficulty": "medium",
      "input": "I can't seem to log into my account even after resetting my password",
      "expected_intent": "technical_support",
      "expected_answer": "Let me help you troubleshoot the login issue. First, please clear your browser cache and cookies, then try logging in again.",
      "expected_answer_semantic": ["troubleshoot", "clear cache", "cookies", "try again"],
      "metadata": {
        "user_type": "existing",
        "context_required": true,
        "requires_escalation": false
      }
    },
    {
      "id": "TC003",
      "category": "edge_case",
      "difficulty": "hard",
      "input": "yo whats the deal with my bill being so high lol",
      "expected_intent": "billing",
      "expected_answer": "I understand you have concerns about your bill. Let me review your account to identify any unexpected charges.",
      "expected_answer_semantic": ["concerns", "bill", "review", "charges"],
      "metadata": {
        "user_type": "existing",
        "context_required": true,
        "tone": "informal",
        "requires_empathy": true
      }
    }
  ]
}
```

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ã‚«ãƒãƒ¬ãƒƒã‚¸

**ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒãƒ©ãƒ³ã‚¹**
```python
def analyze_test_coverage(test_cases: List[Dict]) -> Dict:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’åˆ†æ"""
    categories = {}
    difficulties = {}

    for case in test_cases:
        # ã‚«ãƒ†ã‚´ãƒª
        cat = case.get("category", "unknown")
        categories[cat] = categories.get(cat, 0) + 1

        # é›£æ˜“åº¦
        diff = case.get("difficulty", "unknown")
        difficulties[diff] = difficulties.get(diff, 0) + 1

    total = len(test_cases)

    return {
        "total_cases": total,
        "by_category": {
            cat: {"count": count, "percentage": count/total*100}
            for cat, count in categories.items()
        },
        "by_difficulty": {
            diff: {"count": count, "percentage": count/total*100}
            for diff, count in difficulties.items()
        }
    }

# æ¨å¥¨ãƒãƒ©ãƒ³ã‚¹
"""
ã‚«ãƒ†ã‚´ãƒªåˆ¥:
- å„ã‚«ãƒ†ã‚´ãƒª: 20-30% (å‡ç­‰åˆ†æ•£)
- Edge cases: 10-15% (ååˆ†ãªç•°å¸¸ç³»ã‚«ãƒãƒ¬ãƒƒã‚¸)

é›£æ˜“åº¦åˆ¥:
- Easy: 40-50% (åŸºæœ¬æ©Ÿèƒ½ã®ç¢ºèª)
- Medium: 30-40% (å®Ÿç”¨çš„ãªã‚±ãƒ¼ã‚¹)
- Hard: 10-20% (ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã¨è¤‡é›‘ãªã‚·ãƒŠãƒªã‚ª)
"""
```

## ğŸ“ˆ çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œè¨¼

### è¤‡æ•°å›å®Ÿè¡Œã®é‡è¦æ€§

**ãªãœè¤‡æ•°å›å®Ÿè¡ŒãŒå¿…è¦ã‹**:
1. **ãƒ©ãƒ³ãƒ€ãƒ æ€§ã®è€ƒæ…®**: LLM å‡ºåŠ›ã«ã¯ç¢ºç‡çš„ãªå¤‰å‹•ãŒã‚ã‚‹
2. **å¤–ã‚Œå€¤ã®æ¤œå‡º**: ä¸€æ™‚çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ãªã©ã®å½±éŸ¿ã‚’æ’é™¤
3. **ä¿¡é ¼åŒºé–“ã®è¨ˆç®—**: æ”¹å–„ãŒçµ±è¨ˆçš„ã«æœ‰æ„ã‹ã‚’åˆ¤æ–­

**æ¨å¥¨å®Ÿè¡Œå›æ•°**:
- **é–‹ç™ºä¸­**: 3å›ï¼ˆè¿…é€Ÿãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼‰
- **è©•ä¾¡æ™‚**: 5å›ï¼ˆãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸä¿¡é ¼æ€§ï¼‰
- **æœ¬ç•ªå‰**: 10-20å›ï¼ˆé«˜ã„çµ±è¨ˆçš„ä¿¡é ¼æ€§ï¼‰

### çµ±è¨ˆåˆ†æ

```python
import numpy as np
from scipy import stats

def statistical_analysis(
    baseline_results: List[float],
    improved_results: List[float],
    alpha: float = 0.05
) -> Dict:
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ”¹å–„ç‰ˆã®çµ±è¨ˆçš„æ¯”è¼ƒ"""

    # åŸºæœ¬çµ±è¨ˆé‡
    baseline_stats = {
        "mean": np.mean(baseline_results),
        "std": np.std(baseline_results),
        "median": np.median(baseline_results),
        "min": np.min(baseline_results),
        "max": np.max(baseline_results)
    }

    improved_stats = {
        "mean": np.mean(improved_results),
        "std": np.std(improved_results),
        "median": np.median(improved_results),
        "min": np.min(improved_results),
        "max": np.max(improved_results)
    }

    # tæ¤œå®šï¼ˆå¯¾å¿œãªã—ï¼‰
    t_statistic, p_value = stats.ttest_ind(improved_results, baseline_results)

    # åŠ¹æœé‡ï¼ˆCohen's dï¼‰
    pooled_std = np.sqrt(
        ((len(baseline_results) - 1) * baseline_stats["std"]**2 +
         (len(improved_results) - 1) * improved_stats["std"]**2) /
        (len(baseline_results) + len(improved_results) - 2)
    )
    cohens_d = (improved_stats["mean"] - baseline_stats["mean"]) / pooled_std

    # æ”¹å–„ç‡
    improvement_pct = (
        (improved_stats["mean"] - baseline_stats["mean"]) /
        baseline_stats["mean"] * 100
    )

    # ä¿¡é ¼åŒºé–“ï¼ˆ95%ï¼‰
    ci_baseline = stats.t.interval(
        0.95,
        len(baseline_results) - 1,
        loc=baseline_stats["mean"],
        scale=stats.sem(baseline_results)
    )

    ci_improved = stats.t.interval(
        0.95,
        len(improved_results) - 1,
        loc=improved_stats["mean"],
        scale=stats.sem(improved_results)
    )

    # çµ±è¨ˆçš„æœ‰æ„æ€§ã®åˆ¤å®š
    is_significant = p_value < alpha

    # åŠ¹æœã®å¤§ãã•ã®è§£é‡ˆ
    effect_size_interpretation = (
        "small" if abs(cohens_d) < 0.5 else
        "medium" if abs(cohens_d) < 0.8 else
        "large"
    )

    return {
        "baseline": baseline_stats,
        "improved": improved_stats,
        "comparison": {
            "improvement_pct": improvement_pct,
            "t_statistic": t_statistic,
            "p_value": p_value,
            "is_significant": is_significant,
            "cohens_d": cohens_d,
            "effect_size": effect_size_interpretation
        },
        "confidence_intervals": {
            "baseline": ci_baseline,
            "improved": ci_improved
        }
    }

# ä½¿ç”¨ä¾‹
baseline_accuracy = [73.0, 75.0, 77.0, 74.0, 76.0]  # 5å›ã®å®Ÿè¡Œçµæœ
improved_accuracy = [85.0, 87.0, 86.0, 88.0, 84.0]  # æ”¹å–„å¾Œã®5å›ã®å®Ÿè¡Œçµæœ

analysis = statistical_analysis(baseline_accuracy, improved_accuracy)
print(f"Improvement: {analysis['comparison']['improvement_pct']:.1f}%")
print(f"P-value: {analysis['comparison']['p_value']:.4f}")
print(f"Significant: {analysis['comparison']['is_significant']}")
print(f"Effect size: {analysis['comparison']['effect_size']}")
```

## ğŸ¯ è©•ä¾¡ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ä¸€è²«æ€§ã®ç¢ºä¿

**åŒã˜æ¡ä»¶ã§ã®è©•ä¾¡**:
```python
class EvaluationConfig:
    """è©•ä¾¡è¨­å®šã‚’å›ºå®šã—ã¦ä¸€è²«æ€§ã‚’ç¢ºä¿"""

    def __init__(self):
        self.test_cases_path = "tests/evaluation/test_cases.json"
        self.seed = 42  # å†ç¾æ€§ã®ãŸã‚
        self.iterations = 5
        self.timeout = 30  # seconds
        self.model = "claude-3-5-sonnet-20241022"

    def load_test_cases(self) -> List[Dict]:
        """åŒã˜ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã‚€"""
        with open(self.test_cases_path) as f:
            data = json.load(f)
        return data["test_cases"]

# ä½¿ç”¨
config = EvaluationConfig()
test_cases = config.load_test_cases()
# ã™ã¹ã¦ã®è©•ä¾¡ã§åŒã˜ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ä½¿ç”¨
```

### 2. æ®µéšçš„ãªè©•ä¾¡

**å°ã•ãå§‹ã‚ã¦å¾ã€…ã«æ‹¡å¤§**:
```python
# Phase 1: Quick check (3 cases, 1 iteration)
quick_results = evaluate(test_cases[:3], iterations=1)

if quick_results["accuracy"] > baseline["accuracy"]:
    # Phase 2: Medium check (10 cases, 3 iterations)
    medium_results = evaluate(test_cases[:10], iterations=3)

    if medium_results["accuracy"] > baseline["accuracy"]:
        # Phase 3: Full evaluation (all cases, 5 iterations)
        full_results = evaluate(test_cases, iterations=5)
```

### 3. è©•ä¾¡çµæœã®è¨˜éŒ²

**æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ­ã‚°**:
```python
import json
from datetime import datetime
from pathlib import Path

def save_evaluation_result(
    results: Dict,
    version: str,
    output_dir: Path = Path("evaluation_results")
):
    """è©•ä¾¡çµæœã‚’ä¿å­˜"""
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{version}_{timestamp}.json"

    full_results = {
        "version": version,
        "timestamp": timestamp,
        "metrics": results,
        "config": {
            "model": "claude-3-5-sonnet-20241022",
            "test_cases": len(test_cases),
            "iterations": 5
        }
    }

    with open(output_dir / filename, "w") as f:
        json.dump(full_results, f, indent=2)

    print(f"Results saved to: {output_dir / filename}")

# ä½¿ç”¨
save_evaluation_result(results, version="baseline")
save_evaluation_result(results, version="iteration_1")
```

### 4. å¯è¦–åŒ–

**çµæœã®å¯è¦–åŒ–**:
```python
import matplotlib.pyplot as plt

def visualize_improvement(
    baseline: Dict,
    iterations: List[Dict],
    metrics: List[str] = ["accuracy", "latency", "cost"]
):
    """æ”¹å–„ã®æ¨ç§»ã‚’å¯è¦–åŒ–"""
    fig, axes = plt.subplots(1, len(metrics), figsize=(15, 5))

    for idx, metric in enumerate(metrics):
        ax = axes[idx]

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        x = ["Baseline"] + [f"Iter {i+1}" for i in range(len(iterations))]
        y = [baseline[metric]] + [it[metric] for it in iterations]

        # ãƒ—ãƒ­ãƒƒãƒˆ
        ax.plot(x, y, marker='o', linewidth=2)
        ax.set_title(f"{metric.capitalize()} Progress")
        ax.set_ylabel(metric.capitalize())
        ax.grid(True, alpha=0.3)

        # ç›®æ¨™ç·š
        if metric in baseline.get("goals", {}):
            goal = baseline["goals"][metric]
            ax.axhline(y=goal, color='r', linestyle='--', label='Goal')
            ax.legend()

    plt.tight_layout()
    plt.savefig("evaluation_results/improvement_progress.png")
    print("Visualization saved to: evaluation_results/improvement_progress.png")
```

## ğŸ“‹ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```markdown
# è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ - [Version/Iteration]

å®Ÿè¡Œæ—¥æ™‚: 2024-11-24 12:00:00
å®Ÿè¡Œè€…: Claude Code (fine-tune skill)

## è¨­å®š

- **ãƒ¢ãƒ‡ãƒ«**: claude-3-5-sonnet-20241022
- **ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°**: 20
- **å®Ÿè¡Œå›æ•°**: 5
- **è©•ä¾¡æœŸé–“**: 10åˆ†

## çµæœã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | å¹³å‡ | æ¨™æº–åå·® | 95% CI | ç›®æ¨™ | é”æˆç‡ |
|------|------|----------|--------|------|--------|
| Accuracy | 86.0% | 2.1% | [83.9%, 88.1%] | 90.0% | 95.6% |
| Latency | 2.4s | 0.3s | [2.1s, 2.7s] | 2.0s | 83.3% |
| Cost | $0.014 | $0.001 | [$0.013, $0.015] | $0.010 | 71.4% |

## è©³ç´°åˆ†æ

### Accuracy
- **æ”¹å–„**: +11.0% (75.0% â†’ 86.0%)
- **çµ±è¨ˆçš„æœ‰æ„æ€§**: p < 0.01 âœ…
- **åŠ¹æœé‡**: Cohen's d = 2.3 (large)

### Latency
- **æ”¹å–„**: -0.1s (2.5s â†’ 2.4s)
- **çµ±è¨ˆçš„æœ‰æ„æ€§**: p = 0.12 âŒï¼ˆæœ‰æ„ã§ãªã„ï¼‰
- **åŠ¹æœé‡**: Cohen's d = 0.3 (small)

## ã‚¨ãƒ©ãƒ¼åˆ†æ

- **ç·ã‚¨ãƒ©ãƒ¼æ•°**: 0
- **ã‚¨ãƒ©ãƒ¼ç‡**: 0.0%
- **ãƒªãƒˆãƒ©ã‚¤ç‡**: 0.0%

## æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. âœ… Accuracy ãŒå¤§å¹…ã«å‘ä¸Š â†’ ç¶™ç¶š
2. âš ï¸ Latency ã¯æ”¹å–„ãŒå°ã•ã„ â†’ æ¬¡ã® iteration ã§ focus
3. âš ï¸ Cost ã¯ã¾ã ç›®æ¨™æœªé” â†’ max_tokens åˆ¶é™ã‚’æ¤œè¨
```

## ã¾ã¨ã‚

åŠ¹æœçš„ãªè©•ä¾¡ã®ãŸã‚ã«ï¼š
- âœ… **è¤‡æ•°ã®æŒ‡æ¨™**: å“è³ªã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ã‚³ã‚¹ãƒˆã€ä¿¡é ¼æ€§
- âœ… **çµ±è¨ˆçš„æ¤œè¨¼**: è¤‡æ•°å›å®Ÿè¡Œã¨æœ‰æ„æ€§æ¤œå®š
- âœ… **ä¸€è²«æ€§**: åŒã˜ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€åŒã˜æ¡ä»¶
- âœ… **å¯è¦–åŒ–**: ã‚°ãƒ©ãƒ•ã¨è¡¨ã§æ”¹å–„ã‚’è¿½è·¡
- âœ… **æ–‡æ›¸åŒ–**: è©•ä¾¡çµæœã¨åˆ†æã‚’è¨˜éŒ²
