# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè·µä¾‹é›†

LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä½¿ç”¨ã™ã‚‹å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé›†ã€‚

## ğŸ“‹ ç›®æ¬¡

- [Phase 1: æº–å‚™ã¨åˆ†æã®ä¾‹](#phase-1-æº–å‚™ã¨åˆ†æã®ä¾‹)
- [Phase 2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ä¾‹](#phase-2-ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ä¾‹)
- [Phase 3: åå¾©çš„æ”¹å–„ã®ä¾‹](#phase-3-åå¾©çš„æ”¹å–„ã®ä¾‹)
- [Phase 4: å®Œäº†ã¨æ–‡æ›¸åŒ–ã®ä¾‹](#phase-4-å®Œäº†ã¨æ–‡æ›¸åŒ–ã®ä¾‹)

---

## Phase 1: æº–å‚™ã¨åˆ†æã®ä¾‹

### Example 1.1: fine-tune.md ã®æ§‹é€ ä¾‹

**ãƒ•ã‚¡ã‚¤ãƒ«**: `.langgraph-master/fine-tune.md`

```markdown
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç›®æ¨™

## æœ€é©åŒ–ç›®æ¨™

- **Accuracy**: ãƒ¦ãƒ¼ã‚¶ãƒ¼æ„å›³ã®åˆ†é¡ç²¾åº¦ã‚’ 90%ä»¥ä¸Šã«å‘ä¸Š
- **Latency**: å¿œç­”æ™‚é–“ã‚’ 2.0 ç§’ä»¥ä¸‹ã«çŸ­ç¸®
- **Cost**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆã‚’ $0.010 ä»¥ä¸‹ã«å‰Šæ¸›

## è©•ä¾¡æ–¹æ³•

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹

- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: tests/evaluation/test_cases.json (20 ã‚±ãƒ¼ã‚¹)
- **å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**: uv run python -m src.evaluate
- **è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: tests/evaluation/evaluator.py

### è©•ä¾¡æŒ‡æ¨™

#### Accuracyï¼ˆæ­£è§£ç‡ï¼‰

- **è¨ˆç®—æ–¹æ³•**: (æ­£è§£æ•° / ç·ã‚±ãƒ¼ã‚¹æ•°) Ã— 100
- **ç›®æ¨™å€¤**: 90%ä»¥ä¸Š

#### Latencyï¼ˆå¿œç­”æ™‚é–“ï¼‰

- **è¨ˆç®—æ–¹æ³•**: å„å®Ÿè¡Œã®å¹³å‡æ™‚é–“
- **ç›®æ¨™å€¤**: 2.0 ç§’ä»¥ä¸‹

#### Costï¼ˆã‚³ã‚¹ãƒˆï¼‰

- **è¨ˆç®—æ–¹æ³•**: ç· API ã‚³ã‚¹ãƒˆ / ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
- **ç›®æ¨™å€¤**: $0.010 ä»¥ä¸‹

## åˆæ ¼åŸºæº–

ã™ã¹ã¦ã®è©•ä¾¡æŒ‡æ¨™ãŒç›®æ¨™å€¤ã‚’é”æˆã™ã‚‹ã“ã¨ã€‚
```

### Example 1.2: æœ€é©åŒ–ç®‡æ‰€ãƒªã‚¹ãƒˆã®ä¾‹

```markdown
# æœ€é©åŒ–å¯¾è±¡ãƒãƒ¼ãƒ‰

## Node: analyze_intent

### åŸºæœ¬æƒ…å ±

- **ãƒ•ã‚¡ã‚¤ãƒ«**: src/nodes/analyzer.py:25-45
- **å½¹å‰²**: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®æ„å›³ã‚’åˆ†é¡
- **LLM ãƒ¢ãƒ‡ãƒ«**: claude-3-5-sonnet-20241022
- **ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: temperature=1.0, max_tokens=default

### ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

\```python
SystemMessage(content="You are an intent analyzer. Analyze user input.")
HumanMessage(content=f"Analyze: {user_input}")
\```

### å•é¡Œç‚¹

1. **æ›–æ˜§ãªæŒ‡ç¤º**: "Analyze" ã®å…·ä½“çš„ãªåŸºæº–ãŒä¸æ˜
2. **Few-shot ãªã—**: æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹ãŒãªã„
3. **å‡ºåŠ›å½¢å¼æœªå®šç¾©**: è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆã§æ§‹é€ åŒ–ã•ã‚Œã¦ã„ãªã„
4. **é«˜ temperature**: 1.0 ã¯åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã¯é«˜ã™ãã‚‹

### æ”¹å–„æ¡ˆ

1. å…·ä½“çš„ãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªã‚’æ˜è¨˜
2. Few-shot examples ã‚’ 3-5 å€‹è¿½åŠ 
3. JSON å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®š
4. temperature ã‚’ 0.3-0.5 ã«ä¸‹ã’ã‚‹

### æ¨å®šæ”¹å–„åŠ¹æœ

- **Accuracy**: +10-15% (ç¾çŠ¶ã®èª¤åˆ†é¡ 20% â†’ 5-10%)
- **Latency**: Â±0 (å¤‰åŒ–ãªã—)
- **Cost**: Â±0 (å¤‰åŒ–ãªã—)

### å„ªå…ˆåº¦

â­â­â­â­â­ (æœ€å„ªå…ˆ) - accuracy å‘ä¸Šã¸ã®ç›´æ¥çš„ãªå½±éŸ¿

---

## Node: generate_response

### åŸºæœ¬æƒ…å ±

- **ãƒ•ã‚¡ã‚¤ãƒ«**: src/nodes/generator.py:45-68
- **å½¹å‰²**: æœ€çµ‚çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘å¿œç­”ã‚’ç”Ÿæˆ
- **LLM ãƒ¢ãƒ‡ãƒ«**: claude-3-5-sonnet-20241022
- **ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: temperature=0.7, max_tokens=default

### ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

\```python
ChatPromptTemplate.from_messages([
    ("system", "Generate helpful response based on context."),
    ("human", "{context}\n\nQuestion: {question}")
])
\```

### å•é¡Œç‚¹

1. **å†—é•·æ€§åˆ¶å¾¡ãªã—**: ç°¡æ½”æ€§ã®æŒ‡ç¤ºãŒãªã„
2. **max_tokens æœªè¨­å®š**: ä¸å¿…è¦ã«é•·ã„å‡ºåŠ›ã®å¯èƒ½æ€§
3. **å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«æœªå®šç¾©**: ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã®æŒ‡å®šãŒãªã„

### æ”¹å–„æ¡ˆ

1. "ç°¡æ½”ã«" "2-3 æ–‡ã§" ãªã©ã®é•·ã•æŒ‡ç¤ºã‚’è¿½åŠ 
2. max_tokens ã‚’ 500 ã«åˆ¶é™
3. å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ˜ç¢ºåŒ–ï¼ˆ"è¦ªã—ã¿ã‚„ã™ã" "å°‚é–€çš„ã«" ãªã©ï¼‰

### æ¨å®šæ”¹å–„åŠ¹æœ

- **Accuracy**: Â±0 (å¤‰åŒ–ãªã—)
- **Latency**: -0.3-0.5s (å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ã«ã‚ˆã‚‹)
- **Cost**: -20-30% (ãƒˆãƒ¼ã‚¯ãƒ³æ•°å‰Šæ¸›ã«ã‚ˆã‚‹)

### å„ªå…ˆåº¦

â­â­â­ (ä¸­) - latency ã¨ cost ã®æ”¹å–„
```

### Example 1.3: Serena MCP ã§ã®ã‚³ãƒ¼ãƒ‰æ¤œç´¢ä¾‹

```python
# LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¤œç´¢
from mcp_serena import find_symbol, find_referencing_symbols

# Step 1: ChatAnthropic ã®ä½¿ç”¨ç®‡æ‰€ã‚’æ¤œç´¢
chat_anthropic_usages = find_symbol(
    name_path="ChatAnthropic",
    substring_matching=True,
    include_body=False
)

print(f"Found {len(chat_anthropic_usages)} ChatAnthropic usages")

# Step 2: å„ä½¿ç”¨ç®‡æ‰€ã®è©³ç´°ã‚’èª¿æŸ»
for usage in chat_anthropic_usages:
    print(f"\nFile: {usage.relative_path}:{usage.line_start}")
    print(f"Context: {usage.name_path}")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ç®‡æ‰€ã‚’ç‰¹å®š
    references = find_referencing_symbols(
        name_path=usage.name,
        relative_path=usage.relative_path
    )

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚€å¯èƒ½æ€§ã®ã‚ã‚‹ç®‡æ‰€ã‚’è¡¨ç¤º
    for ref in references:
        if "message" in ref.name.lower() or "prompt" in ref.name.lower():
            print(f"  - Potential prompt location: {ref.name_path}")
```

---

## Phase 2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡ã®ä¾‹

### Example 2.1: è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/evaluation/evaluator.py`

```python
import json
import time
from pathlib import Path
from typing import Dict, List

def evaluate_test_cases(test_cases: List[Dict]) -> Dict:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è©•ä¾¡"""
    results = {
        "total_cases": len(test_cases),
        "correct": 0,
        "total_latency": 0.0,
        "total_cost": 0.0,
        "case_results": []
    }

    for case in test_cases:
        start_time = time.time()

        # LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        output = run_langgraph_app(case["input"])

        latency = time.time() - start_time

        # æ­£è§£åˆ¤å®š
        is_correct = output["answer"] == case["expected_answer"]
        if is_correct:
            results["correct"] += 1

        # ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‹ã‚‰ï¼‰
        cost = calculate_cost(output["token_usage"])

        results["total_latency"] += latency
        results["total_cost"] += cost

        results["case_results"].append({
            "case_id": case["id"],
            "correct": is_correct,
            "latency": latency,
            "cost": cost
        })

    # æŒ‡æ¨™ã®è¨ˆç®—
    results["accuracy"] = (results["correct"] / results["total_cases"]) * 100
    results["avg_latency"] = results["total_latency"] / results["total_cases"]
    results["avg_cost"] = results["total_cost"] / results["total_cases"]

    return results

def calculate_cost(token_usage: Dict) -> float:
    """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‹ã‚‰ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
    # Claude 3.5 Sonnet ã®æ–™é‡‘
    INPUT_COST_PER_1M = 3.0  # $3.00 per 1M input tokens
    OUTPUT_COST_PER_1M = 15.0  # $15.00 per 1M output tokens

    input_cost = (token_usage["input_tokens"] / 1_000_000) * INPUT_COST_PER_1M
    output_cost = (token_usage["output_tokens"] / 1_000_000) * OUTPUT_COST_PER_1M

    return input_cost + output_cost

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
    with open("tests/evaluation/test_cases.json") as f:
        test_cases = json.load(f)["test_cases"]

    # è©•ä¾¡å®Ÿè¡Œ
    results = evaluate_test_cases(test_cases)

    # çµæœã‚’ä¿å­˜
    with open("evaluation_results/baseline_run.json", "w") as f:
        json.dump(results, f, indent=2)

    print(f"Accuracy: {results['accuracy']:.1f}%")
    print(f"Avg Latency: {results['avg_latency']:.2f}s")
    print(f"Avg Cost: ${results['avg_cost']:.4f}")
```

### Example 2.2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `scripts/baseline_evaluation.sh`

```bash
#!/bin/bash

ITERATIONS=5
RESULTS_DIR="evaluation_results/baseline"
mkdir -p $RESULTS_DIR

echo "Starting baseline evaluation: $ITERATIONS iterations"

for i in $(seq 1 $ITERATIONS); do
    echo "----------------------------------------"
    echo "Iteration $i/$ITERATIONS"
    echo "----------------------------------------"

    uv run python -m tests.evaluation.evaluator \
        --output "$RESULTS_DIR/run_$i.json" \
        --verbose

    echo "Completed iteration $i"

    # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    if [ $i -lt $ITERATIONS ]; then
        echo "Waiting 5 seconds before next iteration..."
        sleep 5
    fi
done

echo ""
echo "All iterations completed. Aggregating results..."

# çµæœã®é›†è¨ˆ
uv run python -m tests.evaluation.aggregate \
    --input-dir "$RESULTS_DIR" \
    --output "$RESULTS_DIR/summary.json"

echo "Baseline evaluation complete!"
echo "Results saved to: $RESULTS_DIR/summary.json"
```

### Example 2.3: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœãƒ¬ãƒãƒ¼ãƒˆ

```markdown
# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡çµæœ

å®Ÿè¡Œæ—¥æ™‚: 2024-11-24 10:00:00
å®Ÿè¡Œå›æ•°: 5
ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: 20

## è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™     | å¹³å‡   | æ¨™æº–åå·® | æœ€å°å€¤ | æœ€å¤§å€¤ | ç›®æ¨™   | ã‚®ãƒ£ãƒƒãƒ—  |
| -------- | ------ | -------- | ------ | ------ | ------ | --------- |
| Accuracy | 75.0%  | 3.2%     | 70.0%  | 80.0%  | 90.0%  | **-15.0%** |
| Latency  | 2.5s   | 0.4s     | 2.1s   | 3.2s   | 2.0s   | **+0.5s**  |
| Cost/req | $0.015 | $0.002   | $0.013 | $0.018 | $0.010 | **+$0.005** |

## è©³ç´°åˆ†æ

### Accuracy ã®å•é¡Œ

- **ç¾çŠ¶**: 75.0% (ç›®æ¨™: 90.0%)
- **ä¸»ãªèª¤ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³**:
  1. ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡ãƒŸã‚¹: 12 ã‚±ãƒ¼ã‚¹ (60%ã®èª¤ç­”)
  2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ä¸è¶³: 5 ã‚±ãƒ¼ã‚¹ (25%ã®èª¤ç­”)
  3. æ›–æ˜§ãªè³ªå•ã¸ã®å¯¾å¿œ: 3 ã‚±ãƒ¼ã‚¹ (15%ã®èª¤ç­”)

### Latency ã®å•é¡Œ

- **ç¾çŠ¶**: 2.5s (ç›®æ¨™: 2.0s)
- **ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
  1. generate_response ãƒãƒ¼ãƒ‰: å¹³å‡ 1.8s (å…¨ä½“ã® 72%)
  2. analyze_intent ãƒãƒ¼ãƒ‰: å¹³å‡ 0.5s (å…¨ä½“ã® 20%)
  3. ãã®ä»–: å¹³å‡ 0.2s (å…¨ä½“ã® 8%)

### Cost ã®å•é¡Œ

- **ç¾çŠ¶**: $0.015/req (ç›®æ¨™: $0.010/req)
- **ã‚³ã‚¹ãƒˆå†…è¨³**:
  1. generate_response: $0.011 (73%)
  2. analyze_intent: $0.003 (20%)
  3. ãã®ä»–: $0.001 (7%)
- **ä¸»ãªè¦å› **: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šã„ï¼ˆå¹³å‡ 800 tokensï¼‰

## æ”¹å–„ã®æ–¹å‘æ€§

### å„ªå…ˆåº¦ 1: analyze_intent ã®ç²¾åº¦å‘ä¸Š

- **å½±éŸ¿**: Accuracy ã«ç›´æ¥å½±éŸ¿ï¼ˆ-15%ã®ã‚®ãƒ£ãƒƒãƒ—ã® 60%ã‚’å ã‚ã‚‹ï¼‰
- **æ”¹å–„ç­–**: Few-shot examplesã€æ˜ç¢ºãªåˆ†é¡åŸºæº–ã€JSON å‡ºåŠ›å½¢å¼
- **æ¨å®šåŠ¹æœ**: +10-12% accuracy

### å„ªå…ˆåº¦ 2: generate_response ã®åŠ¹ç‡åŒ–

- **å½±éŸ¿**: Latency ã¨ Cost ã®ä¸¡æ–¹ã«å½±éŸ¿
- **æ”¹å–„ç­–**: ç°¡æ½”æ€§ã®æŒ‡ç¤ºã€max_tokens åˆ¶é™ã€temperature èª¿æ•´
- **æ¨å®šåŠ¹æœ**: -0.4s latency, -$0.004 cost
```

---

## Phase 3: åå¾©çš„æ”¹å–„ã®ä¾‹

### Example 3.1: æ”¹å–„å‰å¾Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¯”è¼ƒ

**ãƒãƒ¼ãƒ‰**: analyze_intent

#### Beforeï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰

```python
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=1.0
    )

    messages = [
        SystemMessage(content="You are an intent analyzer. Analyze user input."),
        HumanMessage(content=f"Analyze: {state['user_input']}")
    ]

    response = llm.invoke(messages)
    state["intent"] = response.content
    return state
```

**å•é¡Œç‚¹**:
- æ›–æ˜§ãªæŒ‡ç¤º
- Few-shot ãªã—
- è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›
- é«˜ã„ temperature

**çµæœ**: Accuracy 75%

#### Afterï¼ˆIteration 1ï¼‰

```python
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=0.3  # åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã¯ä½ã‚ã® temperature
    )

    # æ˜ç¢ºãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªã¨ few-shot examples
    system_prompt = """You are an intent classifier for a customer support chatbot.

Classify user input into one of these categories:
- "product_inquiry": Questions about products or services
- "technical_support": Technical issues or troubleshooting
- "billing": Payment, invoicing, or billing questions
- "general": General questions or chitchat

Output ONLY a valid JSON object with this structure:
{
  "intent": "<category>",
  "confidence": <0.0-1.0>,
  "reasoning": "<brief explanation>"
}

Examples:

Input: "How much does the premium plan cost?"
Output: {"intent": "product_inquiry", "confidence": 0.95, "reasoning": "Question about product pricing"}

Input: "I can't log into my account"
Output: {"intent": "technical_support", "confidence": 0.9, "reasoning": "Authentication issue"}

Input: "Why was I charged twice?"
Output: {"intent": "billing", "confidence": 0.95, "reasoning": "Question about billing charges"}

Input: "Hello, how are you?"
Output: {"intent": "general", "confidence": 0.85, "reasoning": "General greeting"}

Input: "What's the return policy?"
Output: {"intent": "product_inquiry", "confidence": 0.9, "reasoning": "Question about product policy"}
"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"Input: {state['user_input']}\nOutput:")
    ]

    response = llm.invoke(messages)

    # JSON ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
    try:
        intent_data = json.loads(response.content)
        state["intent"] = intent_data["intent"]
        state["confidence"] = intent_data["confidence"]
    except json.JSONDecodeError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        state["intent"] = "general"
        state["confidence"] = 0.5

    return state
```

**æ”¹å–„ç‚¹**:
- âœ… temperature: 1.0 â†’ 0.3
- âœ… æ˜ç¢ºãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªï¼ˆ4 ã¤ã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼‰
- âœ… Few-shot examplesï¼ˆ5 å€‹è¿½åŠ ï¼‰
- âœ… JSON å‡ºåŠ›å½¢å¼ï¼ˆæ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ï¼‰
- âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆJSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

**çµæœ**: Accuracy 86% (+11%)

### Example 3.2: å„ªå…ˆé †ä½ä»˜ã‘ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

```markdown
## æ”¹å–„å„ªå…ˆé †ä½ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ãƒãƒ¼ãƒ‰             | å½±éŸ¿åº¦       | å®Ÿç¾å¯èƒ½æ€§   | å®Ÿè£…ã‚³ã‚¹ãƒˆ   | ç·åˆã‚¹ã‚³ã‚¢ | å„ªå…ˆåº¦ |
| ------------------ | ------------ | ------------ | ------------ | ---------- | ------ |
| analyze_intent     | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­   | 14/15      | 1 ä½   |
| generate_response  | â­â­â­â­   | â­â­â­â­   | â­â­â­â­   | 12/15      | 2 ä½   |
| retrieve_context   | â­â­       | â­â­â­     | â­â­â­     | 8/15       | 3 ä½   |

### è©³ç´°åˆ†æ

#### 1 ä½: analyze_intent ãƒãƒ¼ãƒ‰

- **å½±éŸ¿åº¦**: â­â­â­â­â­
  - Accuracy ã«ç›´æ¥å½±éŸ¿ï¼ˆ-15%ã‚®ãƒ£ãƒƒãƒ—ã® 60%ã‚’å ã‚ã‚‹ï¼‰
  - ä¸‹æµã®ãƒãƒ¼ãƒ‰ã«ã‚‚å½±éŸ¿ï¼ˆèª¤åˆ†é¡ã«ã‚ˆã‚‹é€£é–ã‚¨ãƒ©ãƒ¼ï¼‰

- **å®Ÿç¾å¯èƒ½æ€§**: â­â­â­â­â­
  - Few-shot examples ã«ã‚ˆã‚‹æ”¹å–„ãŒæœŸå¾…ã§ãã‚‹
  - é¡ä¼¼äº‹ä¾‹ã§ +10-15%ã®æ”¹å–„å®Ÿç¸¾ã‚ã‚Š

- **å®Ÿè£…ã‚³ã‚¹ãƒˆ**: â­â­â­â­
  - å®Ÿè£…æ™‚é–“: 30-60 åˆ†
  - ãƒ†ã‚¹ãƒˆæ™‚é–“: 30 åˆ†
  - ãƒªã‚¹ã‚¯: ä½

**Iteration 1 ã®å¯¾è±¡**: analyze_intent ãƒãƒ¼ãƒ‰

#### 2 ä½: generate_response ãƒãƒ¼ãƒ‰

- **å½±éŸ¿åº¦**: â­â­â­â­
  - Latency ã¨ Cost ã®ä¸»è¦å› ï¼ˆå…¨ä½“ã® 70%ä»¥ä¸Šï¼‰
  - Accuracy ã¸ã®ç›´æ¥å½±éŸ¿ã¯å°ã•ã„

- **å®Ÿç¾å¯èƒ½æ€§**: â­â­â­â­
  - max_tokens åˆ¶é™ã§ç¢ºå®Ÿã«æ”¹å–„
  - ç°¡æ½”æ€§ã®æŒ‡ç¤ºã§å“è³ªç¶­æŒå¯èƒ½

- **å®Ÿè£…ã‚³ã‚¹ãƒˆ**: â­â­â­â­
  - å®Ÿè£…æ™‚é–“: 20-30 åˆ†
  - ãƒ†ã‚¹ãƒˆæ™‚é–“: 30 åˆ†
  - ãƒªã‚¹ã‚¯: ä½

**Iteration 2 ã®å¯¾è±¡**: generate_response ãƒãƒ¼ãƒ‰
```

### Example 3.3: Iteration çµæœãƒ¬ãƒãƒ¼ãƒˆ

```markdown
# Iteration 1 è©•ä¾¡çµæœ

å®Ÿè¡Œæ—¥æ™‚: 2024-11-24 12:00:00
å¤‰æ›´å†…å®¹: analyze_intent ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

## çµæœæ¯”è¼ƒ

| æŒ‡æ¨™     | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | Iteration 1 | å¤‰åŒ–    | å¤‰åŒ–ç‡  | ç›®æ¨™   | é”æˆç‡  |
| -------- | ------------ | ----------- | ------- | ------- | ------ | ------- |
| **Accuracy** | 75.0%        | **86.0%**   | **+11.0%** | +14.7%  | 90.0%  | 95.6%   |
| **Latency**  | 2.5s         | 2.4s        | -0.1s   | -4.0%   | 2.0s   | 80.0%   |
| **Cost/req** | $0.015       | $0.014      | -$0.001 | -6.7%   | $0.010 | 71.4%   |

## è©³ç´°åˆ†æ

### Accuracy ã®æ”¹å–„

- **å‘ä¸Š**: +11.0% (75.0% â†’ 86.0%)
- **æ®‹ã‚Šã‚®ãƒ£ãƒƒãƒ—**: 4.0% (ç›®æ¨™ 90.0%)
- **æ”¹å–„ã§ããŸã‚±ãƒ¼ã‚¹**: ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡ãƒŸã‚¹ãŒ 12 â†’ 3 ã‚±ãƒ¼ã‚¹ã«æ¸›å°‘
- **ã¾ã æ”¹å–„ãŒå¿…è¦**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ä¸è¶³ã®ã‚±ãƒ¼ã‚¹ï¼ˆ5 ã‚±ãƒ¼ã‚¹ï¼‰

### Latency ã®è‹¥å¹²æ”¹å–„

- **å‘ä¸Š**: -0.1s (2.5s â†’ 2.4s)
- **ä¸»ãªè¦å› **: analyze_intent ã®æ¸©åº¦ä¸‹é™ã«ã‚ˆã‚Šå‡ºåŠ›ãŒç°¡æ½”ã«ãªã£ãŸ
- **æ®‹ã‚Šãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: generate_response (å¹³å‡ 1.8s)

### Cost ã®è‹¥å¹²å‰Šæ¸›

- **å‰Šæ¸›**: -$0.001 (6.7%å‰Šæ¸›)
- **è¦å› **: analyze_intent ã®å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
- **ä¸»ãªã‚³ã‚¹ãƒˆ**: generate_response ãŒä¾ç„¶ã¨ã—ã¦ 73%ã‚’å ã‚ã‚‹

## çµ±è¨ˆçš„æœ‰æ„æ€§

- **t æ¤œå®š**: p < 0.01 âœ…ï¼ˆçµ±è¨ˆçš„ã«æœ‰æ„ï¼‰
- **åŠ¹æœé‡**: Cohen's d = 2.3 (large effect)
- **ä¿¡é ¼åŒºé–“**: [83.9%, 88.1%] (95% CI)

## æ¬¡ã® Iteration ã®æ–¹é‡

### å„ªå…ˆåº¦ 1: generate_response ã®æœ€é©åŒ–

- **ç›®æ¨™**: Latency ã‚’ 1.8s â†’ 1.4sã€Cost ã‚’ $0.011 â†’ $0.007
- **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
  1. ç°¡æ½”æ€§ã®æŒ‡ç¤ºè¿½åŠ 
  2. max_tokens ã‚’ 500 ã«åˆ¶é™
  3. temperature ã‚’ 0.7 â†’ 0.5 ã«èª¿æ•´

### å„ªå…ˆåº¦ 2: Accuracy ã®æœ€å¾Œã® 4%å‘ä¸Š

- **ç›®æ¨™**: 86.0% â†’ 90.0%ä»¥ä¸Š
- **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã‚’æ”¹å–„ï¼ˆretrieve_context ãƒãƒ¼ãƒ‰ï¼‰

## åˆ¤å®š

âœ… **ç¶™ç¶š** â†’ Iteration 2 ã«é€²ã‚€

ç†ç”±:
- Accuracy ãŒå¤§å¹…ã«å‘ä¸Šã—ãŸãŒã€ã¾ã ç›®æ¨™æœªé”
- Latency ã¨ Cost ã‚‚æ”¹å–„ã®ä½™åœ°ã‚ã‚Š
- æ˜ç¢ºãªæ”¹å–„æ–¹é‡ãŒç«‹ã£ã¦ã„ã‚‹
```

---

## Phase 4: å®Œäº†ã¨æ–‡æ›¸åŒ–ã®ä¾‹

### Example 4.1: æœ€çµ‚è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

```markdown
# LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
å®Ÿæ–½æœŸé–“: 2024-11-24 10:00 - 2024-11-24 15:00 (5 æ™‚é–“)
å®Ÿæ–½è€…: Claude Code (fine-tune skill)

## ğŸ¯ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

æœ¬ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€LangGraph ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚’å®Ÿæ–½ã—ã€ä»¥ä¸‹ã®æˆæœã‚’é”æˆã—ã¾ã—ãŸï¼š

- âœ… **Accuracy**: 75.0% â†’ 92.0% (+17.0%, ç›®æ¨™ 90%é”æˆ)
- âœ… **Latency**: 2.5s â†’ 1.9s (-24.0%, ç›®æ¨™ 2.0s é”æˆ)
- âš ï¸ **Cost**: $0.015 â†’ $0.011 (-26.7%, ç›®æ¨™ $0.010 æœªé”)

å…¨ 3 iterations ã‚’å®Ÿæ–½ã—ã€3 ã¤ã®æŒ‡æ¨™ã®ã†ã¡ 2 ã¤ã§ç›®æ¨™ã‚’é”æˆã—ã¾ã—ãŸã€‚

## ğŸ“Š å®Ÿæ–½å†…å®¹ã‚µãƒãƒªãƒ¼

### Iteration æ•°ã¨å®Ÿè¡Œæ™‚é–“

- **Total Iterations**: 3
- **æœ€é©åŒ–ã—ãŸãƒãƒ¼ãƒ‰æ•°**: 2 (analyze_intent, generate_response)
- **è©•ä¾¡å®Ÿè¡Œå›æ•°**: 20 å› (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ 5 å› + å„ iteration å¾Œ 5 å›Ã—3)
- **ç·å®Ÿè¡Œæ™‚é–“**: ç´„ 5 æ™‚é–“

### æœ€çµ‚çµæœ

| æŒ‡æ¨™     | åˆæœŸå€¤ | æœ€çµ‚å€¤ | æ”¹å–„   | æ”¹å–„ç‡  | ç›®æ¨™   | é”æˆçŠ¶æ³  |
| -------- | ------ | ------ | ------ | ------- | ------ | --------- |
| Accuracy | 75.0%  | 92.0%  | +17.0% | +22.7%  | 90.0%  | âœ… 102.2% |
| Latency  | 2.5s   | 1.9s   | -0.6s  | -24.0%  | 2.0s   | âœ… 95.0%  |
| Cost/req | $0.015 | $0.011 | -$0.004| -26.7%  | $0.010 | âš ï¸ 90.9%  |

## ğŸ“ Iteration åˆ¥ã®è©³ç´°

### Iteration 1: analyze_intent ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 11:00
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/analyzer.py:25-45

**å¤‰æ›´å†…å®¹**:
1. temperature: 1.0 â†’ 0.3
2. Few-shot examples ã‚’ 5 å€‹è¿½åŠ 
3. JSON å‡ºåŠ›å½¢å¼ã«æ§‹é€ åŒ–
4. æ˜ç¢ºãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªï¼ˆ4 ã¤ï¼‰ã‚’å®šç¾©

**çµæœ**:
- Accuracy: 75.0% â†’ 86.0% (+11.0%)
- Latency: 2.5s â†’ 2.4s (-0.1s)
- Cost: $0.015 â†’ $0.014 (-$0.001)

**å­¦ã³**: Few-shot examples ã¨æ˜ç¢ºãªå‡ºåŠ›å½¢å¼ãŒ accuracy å‘ä¸Šã«æœ€ã‚‚åŠ¹æœçš„

---

### Iteration 2: generate_response ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 13:00
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/generator.py:45-68

**å¤‰æ›´å†…å®¹**:
1. ç°¡æ½”æ€§ã®æŒ‡ç¤ºã‚’è¿½åŠ ï¼ˆ"2-3 æ–‡ã§å›ç­”"ï¼‰
2. max_tokens: unlimited â†’ 500
3. temperature: 0.7 â†’ 0.5
4. å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ˜ç¢ºåŒ–

**çµæœ**:
- Accuracy: 86.0% â†’ 88.0% (+2.0%)
- Latency: 2.4s â†’ 2.0s (-0.4s)
- Cost: $0.014 â†’ $0.011 (-$0.003)

**å­¦ã³**: max_tokens åˆ¶é™ãŒ latency ã¨ cost å‰Šæ¸›ã«å¤§ããè²¢çŒ®

---

### Iteration 3: analyze_intent ã®è¿½åŠ æ”¹å–„

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 14:30
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/analyzer.py:25-45

**å¤‰æ›´å†…å®¹**:
1. Few-shot examples ã‚’ 5 å€‹ â†’ 10 å€‹ã«å¢—åŠ 
2. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ 
3. confidence threshold ã«ã‚ˆã‚‹å†åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯

**çµæœ**:
- Accuracy: 88.0% â†’ 92.0% (+4.0%)
- Latency: 2.0s â†’ 1.9s (-0.1s)
- Cost: $0.011 â†’ $0.011 (Â±0)

**å­¦ã³**: è¿½åŠ ã® few-shot examples ãŒ accuracy ã®æœ€å¾Œã®å£ã‚’çªç ´

## ğŸ”§ æœ€çµ‚çš„ãªå¤‰æ›´å†…å®¹

### src/nodes/analyzer.py

**å¤‰æ›´è¡Œ**: 25-45

**ä¸»ãªå¤‰æ›´ç‚¹**:
- temperature: 1.0 â†’ 0.3
- Few-shot examples: 0 â†’ 10
- å‡ºåŠ›: è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆ â†’ JSON
- Confidence threshold ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ 

---

### src/nodes/generator.py

**å¤‰æ›´è¡Œ**: 45-68

**ä¸»ãªå¤‰æ›´ç‚¹**:
- temperature: 0.7 â†’ 0.5
- max_tokens: unlimited â†’ 500
- ç°¡æ½”æ€§ã®æ˜ç¢ºãªæŒ‡ç¤ºï¼ˆ"2-3 sentences"ï¼‰
- å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³è¿½åŠ 

## ğŸ“ˆ è©•ä¾¡çµæœã®è©³ç´°

### Test Case åˆ¥ã®æ”¹å–„çŠ¶æ³

| Case ID | Category  | Before      | After       | æ”¹å–„ |
| ------- | --------- | ----------- | ----------- | ---- |
| TC001   | Product   | âŒ Wrong    | âœ… Correct  | âœ…   |
| TC002   | Technical | âŒ Wrong    | âœ… Correct  | âœ…   |
| TC003   | Billing   | âœ… Correct  | âœ… Correct  | -    |
| ...     | ...       | ...         | ...         | ...  |
| TC020   | Technical | âœ… Correct  | âœ… Correct  | -    |

**æ”¹å–„ã•ã‚ŒãŸã‚±ãƒ¼ã‚¹**: 15/20 (75%)
**ç¶­æŒã•ã‚ŒãŸã‚±ãƒ¼ã‚¹**: 5/20 (25%)
**åŠ£åŒ–ã—ãŸã‚±ãƒ¼ã‚¹**: 0/20 (0%)

### Latency ã®å†…è¨³

| ãƒãƒ¼ãƒ‰             | Before | After | å¤‰åŒ–   | å¤‰åŒ–ç‡ |
| ------------------ | ------ | ----- | ------ | ------ |
| analyze_intent     | 0.5s   | 0.4s  | -0.1s  | -20%   |
| retrieve_context   | 0.2s   | 0.2s  | Â±0s    | 0%     |
| generate_response  | 1.8s   | 1.3s  | -0.5s  | -28%   |
| **Total**          | **2.5s** | **1.9s** | **-0.6s** | **-24%** |

### Cost ã®å†…è¨³

| ãƒãƒ¼ãƒ‰             | Before  | After   | å¤‰åŒ–     | å¤‰åŒ–ç‡ |
| ------------------ | ------- | ------- | -------- | ------ |
| analyze_intent     | $0.003  | $0.003  | Â±$0      | 0%     |
| retrieve_context   | $0.001  | $0.001  | Â±$0      | 0%     |
| generate_response  | $0.011  | $0.007  | -$0.004  | -36%   |
| **Total**          | **$0.015** | **$0.011** | **-$0.004** | **-27%** |

## ğŸ’¡ ä»Šå¾Œã®æ¨å¥¨äº‹é …

### çŸ­æœŸï¼ˆ1-2 é€±é–“ï¼‰

1. **Cost ç›®æ¨™ã®é”æˆ**: $0.011 â†’ $0.010
   - ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: Claude 3.5 Haiku ã¸ã®éƒ¨åˆ†ç§»è¡Œã‚’æ¤œè¨
   - æ¨å®šåŠ¹æœ: -$0.002-0.003/req

2. **Accuracy ã®æ›´ãªã‚‹å‘ä¸Š**: 92.0% â†’ 95.0%
   - ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã®åˆ†æã¨ few-shot examples ã®è¿½åŠ 
   - æ¨å®šåŠ¹æœ: +3.0%

### ä¸­æœŸï¼ˆ1-2 ãƒ¶æœˆï¼‰

1. **ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–**
   - simple ãª intent classification ã«ã¯ Haiku ã‚’ä½¿ç”¨
   - complex ãª response generation ã®ã¿ Sonnet ã‚’ä½¿ç”¨
   - æ¨å®šåŠ¹æœ: -30-40% cost, latency ã¸ã®å½±éŸ¿ã¯æœ€å°

2. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ´»ç”¨**
   - System prompts ã¨ few-shot examples ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - æ¨å®šåŠ¹æœ: -50% costï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ï¼‰

### é•·æœŸï¼ˆ3-6 ãƒ¶æœˆï¼‰

1. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨**
   - ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ã® model fine-tuning
   - Few-shot examples ä¸è¦ã§ç°¡æ½”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
   - æ¨å®šåŠ¹æœ: -60% cost, +5% accuracy

## ğŸ“ çµè«–

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã‚’é”æˆã—ã¾ã—ãŸï¼š

âœ… **æˆåŠŸã—ãŸç‚¹**:
1. Accuracy ã®å¤§å¹…å‘ä¸Šï¼ˆ+22.7%ï¼‰- ç›®æ¨™ã‚’ 2.2%è¶…éé”æˆ
2. Latency ã®é¡•è‘—ãªæ”¹å–„ï¼ˆ-24.0%ï¼‰- ç›®æ¨™ã‚’ 5%è¶…éé”æˆ
3. Cost ã®å‰Šæ¸›ï¼ˆ-26.7%ï¼‰- ç›®æ¨™ã«ã‚ã¨ 9.1%

âš ï¸ **èª²é¡Œ**:
1. Cost ç›®æ¨™æœªé”ï¼ˆ$0.011 vs $0.010 ç›®æ¨™ï¼‰- è»½é‡ãƒ¢ãƒ‡ãƒ«ã¸ã®ç§»è¡Œã§å¯¾å¿œå¯èƒ½

ğŸ“ˆ **ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã®å‘ä¸Šï¼ˆaccuracy å‘ä¸Šã«ã‚ˆã‚Šï¼‰
- é‹ç”¨ã‚³ã‚¹ãƒˆã®å‰Šæ¸›ï¼ˆlatency, cost å‰Šæ¸›ã«ã‚ˆã‚Šï¼‰
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ”¹å–„ï¼ˆåŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ï¼‰

ğŸ¯ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**:
1. Cost å‰Šæ¸›ã®ãŸã‚ã®è»½é‡ãƒ¢ãƒ‡ãƒ«ç§»è¡Œã®æ¤œè¨¼
2. ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡
3. æ–°ã—ã„ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¸ã®å±•é–‹

---

ä½œæˆæ—¥æ™‚: 2024-11-24 15:00:00
ä½œæˆè€…: Claude Code (fine-tune skill)
```

### Example 4.2: Git ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¾‹

```bash
# Iteration 1 ã®ã‚³ãƒŸãƒƒãƒˆ
git commit -m "feat(nodes): optimize analyze_intent prompt for accuracy

- Add temperature control (1.0 -> 0.3) for deterministic classification
- Add 5 few-shot examples for intent categories
- Implement JSON structured output format
- Add error handling for JSON parsing failures

Results:
- Accuracy: 75.0% -> 86.0% (+11.0%)
- Latency: 2.5s -> 2.4s (-0.1s)
- Cost: \$0.015 -> \$0.014 (-\$0.001)

Related: fine-tune iteration 1
See: evaluation_results/iteration_1/"

# Iteration 2 ã®ã‚³ãƒŸãƒƒãƒˆ
git commit -m "feat(nodes): optimize generate_response for latency and cost

- Add conciseness guidelines (2-3 sentences)
- Set max_tokens limit to 500
- Adjust temperature (0.7 -> 0.5) for consistency
- Define response style and tone

Results:
- Accuracy: 86.0% -> 88.0% (+2.0%)
- Latency: 2.4s -> 2.0s (-0.4s, -17%)
- Cost: \$0.014 -> \$0.011 (-\$0.003, -21%)

Related: fine-tune iteration 2
See: evaluation_results/iteration_2/"

# æœ€çµ‚ã‚³ãƒŸãƒƒãƒˆ
git commit -m "feat(nodes): finalize fine-tuning with additional improvements

Complete fine-tuning process with 3 iterations:
- analyze_intent: 10 few-shot examples, confidence threshold
- generate_response: conciseness and style optimization

Final Results:
- Accuracy: 75.0% -> 92.0% (+17.0%, goal 90% âœ…)
- Latency: 2.5s -> 1.9s (-0.6s, -24%, goal 2.0s âœ…)
- Cost: \$0.015 -> \$0.011 (-\$0.004, -27%, goal \$0.010 âš ï¸)

Related: fine-tune completion
See: evaluation_results/final_report.md"

# è©•ä¾¡çµæœã®ã‚³ãƒŸãƒƒãƒˆ
git commit -m "docs: add fine-tuning evaluation results and final report

- Baseline evaluation (5 iterations)
- Iteration 1-3 results
- Final comprehensive report
- Statistical analysis and recommendations"
```

---

## ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [SKILL.md](SKILL.md) - ã‚¹ã‚­ãƒ«ã®æ¦‚è¦
- [workflow.md](workflow.md) - ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è©³ç´°
- [evaluation.md](evaluation.md) - è©•ä¾¡æ–¹æ³•
- [prompt_optimization.md](prompt_optimization.md) - æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
