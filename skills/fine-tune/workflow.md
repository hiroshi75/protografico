# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è©³ç´°

LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹éš›ã®è©³ç´°ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¨å®Ÿè·µçš„ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‚

**ğŸ’¡ Tip**: ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã§ä½¿ãˆã‚‹å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ [examples.md](examples.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ“‹ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“åƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: æº–å‚™ã¨åˆ†æ                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. fine-tune.md èª­ã¿è¾¼ã¿ â†’ ç›®æ¨™ã¨è©•ä¾¡åŸºæº–ã®ç†è§£               â”‚
â”‚ 2. Serena ã§æœ€é©åŒ–å¯¾è±¡ã®ç‰¹å®š â†’ LLM å‘¼ã³å‡ºã—ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆåŒ–   â”‚
â”‚ 3. æœ€é©åŒ–ç®‡æ‰€ãƒªã‚¹ãƒˆä½œæˆ â†’ æ”¹å–„å¯èƒ½æ€§ã®è©•ä¾¡                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. è©•ä¾¡ç’°å¢ƒæº–å‚™ â†’ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ                â”‚
â”‚ 5. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š â†’ 3-5å›å®Ÿè¡Œã€çµ±è¨ˆæƒ…å ±åé›†                 â”‚
â”‚ 6. çµæœåˆ†æ â†’ å•é¡Œç‚¹ã®ç‰¹å®šã€æ”¹å–„ä½™åœ°ã®è©•ä¾¡                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: åå¾©çš„æ”¹å–„ (Iteration Loop)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7. å„ªå…ˆé †ä½ä»˜ã‘ â†’ æœ€ã‚‚åŠ¹æœçš„ãªæ”¹å–„ç®‡æ‰€ã®é¸æŠ                  â”‚
â”‚ 8. æ”¹å–„å®Ÿæ–½ â†’ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´                â”‚
â”‚ 9. æ”¹å–„å¾Œè©•ä¾¡ â†’ åŒã˜æ¡ä»¶ã§å†è©•ä¾¡                             â”‚
â”‚ 10. çµæœæ¯”è¼ƒ â†’ æ”¹å–„åŠ¹æœã®æ¸¬å®šã€æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š             â”‚
â”‚ 11. ç¶™ç¶šåˆ¤æ–­ â†’ ç›®æ¨™é”æˆï¼Ÿ Yes â†’ Phase 4 / No â†’ æ¬¡ã® iteration â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 4: å®Œäº†ã¨æ–‡æ›¸åŒ–                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 12. æœ€çµ‚è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ â†’ æ”¹å–„å†…å®¹ã¨çµæœã®ã‚µãƒãƒªãƒ¼           â”‚
â”‚ 13. ã‚³ãƒ¼ãƒ‰ã‚³ãƒŸãƒƒãƒˆ â†’ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Phase 1: æº–å‚™ã¨åˆ†æ

### Step 1: fine-tune.md ã®èª­ã¿è¾¼ã¿ã¨ç†è§£

**ç›®çš„**: æœ€é©åŒ–ã®æ–¹å‘æ€§ã‚’æ˜ç¢ºã«ã™ã‚‹

**å®Ÿè¡Œå†…å®¹**:
```python
# .langgraph-master/fine-tune.md ã‚’èª­ã¿è¾¼ã‚€
file_path = ".langgraph-master/fine-tune.md"
with open(file_path, "r") as f:
    fine_tune_spec = f.read()

# ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡º
# - æœ€é©åŒ–ç›®æ¨™ï¼ˆaccuracy, latency, cost ãªã©ï¼‰
# - è©•ä¾¡æ–¹æ³•ï¼ˆãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€è©•ä¾¡æŒ‡æ¨™ã€è¨ˆç®—æ–¹æ³•ï¼‰
# - åˆæ ¼åŸºæº–ï¼ˆå„æŒ‡æ¨™ã®ç›®æ¨™å€¤ï¼‰
# - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´æ‰€
```

**fine-tune.md ã®å…¸å‹çš„ãªæ§‹é€ **:
```markdown
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç›®æ¨™

## æœ€é©åŒ–ç›®æ¨™
- **Accuracy**: ãƒ¦ãƒ¼ã‚¶ãƒ¼æ„å›³ã®åˆ†é¡ç²¾åº¦ã‚’90%ä»¥ä¸Šã«å‘ä¸Š
- **Latency**: å¿œç­”æ™‚é–“ã‚’2.0ç§’ä»¥ä¸‹ã«çŸ­ç¸®
- **Cost**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆã‚’$0.010ä»¥ä¸‹ã«å‰Šæ¸›

## è©•ä¾¡æ–¹æ³•
- **ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹**: tests/evaluation/test_cases.json (20ã‚±ãƒ¼ã‚¹)
- **å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**: uv run python -m src.evaluate
- **è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**: tests/evaluation/evaluator.py

## è©•ä¾¡æŒ‡æ¨™

### Accuracy
- è¨ˆç®—æ–¹æ³•: (æ­£è§£æ•° / ç·ã‚±ãƒ¼ã‚¹æ•°) Ã— 100
- ç›®æ¨™å€¤: 90%ä»¥ä¸Š

### Latency
- è¨ˆç®—æ–¹æ³•: å„å®Ÿè¡Œã®å¹³å‡æ™‚é–“
- ç›®æ¨™å€¤: 2.0ç§’ä»¥ä¸‹

### Cost
- è¨ˆç®—æ–¹æ³•: ç·API ã‚³ã‚¹ãƒˆ / ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
- ç›®æ¨™å€¤: $0.010ä»¥ä¸‹

## åˆæ ¼åŸºæº–
ã™ã¹ã¦ã®è©•ä¾¡æŒ‡æ¨™ãŒç›®æ¨™å€¤ã‚’é”æˆã™ã‚‹ã“ã¨
```

### Step 2: Serena MCP ã§ã®æœ€é©åŒ–å¯¾è±¡ç‰¹å®š

**ç›®çš„**: LLM ã‚’å‘¼ã³å‡ºã—ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã‚’ç¶²ç¾…çš„ã«ç‰¹å®š

**å®Ÿè¡Œæ‰‹é †**:

1. **LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¤œç´¢**
```python
# Serena MCP: find_symbol ã‚’ä½¿ç”¨
# ChatAnthropic, ChatOpenAI, ChatGoogleGenerativeAI ãªã©ã‚’æ¤œç´¢

patterns = [
    "ChatAnthropic",
    "ChatOpenAI",
    "ChatGoogleGenerativeAI",
    "ChatVertexAI"
]

llm_usages = []
for pattern in patterns:
    results = serena.find_symbol(
        name_path=pattern,
        substring_matching=True,
        include_body=False
    )
    llm_usages.extend(results)
```

2. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ç®‡æ‰€ã®ç‰¹å®š**
```python
# å„ LLM å‘¼ã³å‡ºã—ã«ã¤ã„ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã©ã†æ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã‹èª¿æŸ»
for usage in llm_usages:
    # find_referencing_symbols ã§å‘¨è¾ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    context = serena.find_referencing_symbols(
        name_path=usage.name,
        relative_path=usage.file_path
    )

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç‰¹å®š
    # - ChatPromptTemplate ã®ä½¿ç”¨
    # - SystemMessage, HumanMessage ã®å®šç¾©
    # - f-string ã‚„ format() ã§ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
```

3. **ãƒãƒ¼ãƒ‰ã”ã¨ã®åˆ†æ**
```python
# å„ãƒãƒ¼ãƒ‰é–¢æ•°å†…ã§ã® LLM ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
# - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ˜ç¢ºæ€§
# - Few-shot examples ã®æœ‰ç„¡
# - å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ§‹é€ åŒ–
# - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆtemperature, max_tokens ãªã©ï¼‰
```

**å‡ºåŠ›ä¾‹**:
```markdown
## LLM å‘¼ã³å‡ºã—ç®‡æ‰€ã®åˆ†æ

### 1. analyze_intent ãƒãƒ¼ãƒ‰
- **ãƒ•ã‚¡ã‚¤ãƒ«**: src/nodes/analyzer.py
- **è¡Œç•ªå·**: 25-45
- **LLM**: ChatAnthropic(model="claude-3-5-sonnet-20241022")
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ **:
  ```python
  SystemMessage: "You are an intent analyzer..."
  HumanMessage: f"Analyze: {user_input}"
  ```
- **æ”¹å–„å¯èƒ½æ€§**: â­â­â­â­â­ (é«˜)
  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ›–æ˜§ï¼ˆ"Analyze" ã®åŸºæº–ãŒä¸æ˜ç¢ºï¼‰
  - Few-shot examples ãªã—
  - å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒè‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆ
- **æ¨å®šæ”¹å–„åŠ¹æœ**: Accuracy +10-15%

### 2. generate_response ãƒãƒ¼ãƒ‰
- **ãƒ•ã‚¡ã‚¤ãƒ«**: src/nodes/generator.py
- **è¡Œç•ªå·**: 45-68
- **LLM**: ChatAnthropic(model="claude-3-5-sonnet-20241022")
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ **:
  ```python
  ChatPromptTemplate.from_messages([
      ("system", "Generate helpful response..."),
      ("human", "{context}\n\nQuestion: {question}")
  ])
  ```
- **æ”¹å–„å¯èƒ½æ€§**: â­â­â­ (ä¸­)
  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æ§‹é€ çš„ã ãŒã€ç°¡æ½”æ€§ã®æŒ‡ç¤ºãªã—
  - max_tokens åˆ¶é™ãªã— â†’ å†—é•·ãªå‡ºåŠ›ã®å¯èƒ½æ€§
- **æ¨å®šæ”¹å–„åŠ¹æœ**: Latency -0.3-0.5s, Cost -20-30%
```

### Step 3: æœ€é©åŒ–ç®‡æ‰€ãƒªã‚¹ãƒˆã®ä½œæˆ

**ç›®çš„**: æ”¹å–„ã®å„ªå…ˆé †ä½ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã®æƒ…å ±æ•´ç†

**ãƒªã‚¹ãƒˆä½œæˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```markdown
# æœ€é©åŒ–ç®‡æ‰€ãƒªã‚¹ãƒˆ

## ãƒãƒ¼ãƒ‰: analyze_intent

### åŸºæœ¬æƒ…å ±
- **ãƒ•ã‚¡ã‚¤ãƒ«**: src/nodes/analyzer.py:25-45
- **å½¹å‰²**: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®æ„å›³ã‚’åˆ†é¡
- **LLM ãƒ¢ãƒ‡ãƒ«**: claude-3-5-sonnet-20241022
- **ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: temperature=1.0, max_tokens=default

### ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
```python
SystemMessage(content="You are an intent analyzer. Analyze user input.")
HumanMessage(content=f"Analyze: {user_input}")
```

### å•é¡Œç‚¹
1. **æ›–æ˜§ãªæŒ‡ç¤º**: "Analyze" ã®å…·ä½“çš„ãªåŸºæº–ãŒä¸æ˜
2. **Few-shot ãªã—**: æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹ãŒãªã„
3. **å‡ºåŠ›å½¢å¼æœªå®šç¾©**: è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆã§æ§‹é€ åŒ–ã•ã‚Œã¦ã„ãªã„
4. **é«˜ temperature**: 1.0 ã¯åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã¯é«˜ã™ãã‚‹

### æ”¹å–„æ¡ˆ
1. å…·ä½“çš„ãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªã‚’æ˜è¨˜
2. Few-shot examples ã‚’ 3-5 å€‹è¿½åŠ 
3. JSON å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®š
4. temperature ã‚’ 0.3-0.5 ã«ä¸‹ã’ã‚‹

### æ¨å®šæ”¹å–„åŠ¹æœ
- **Accuracy**: +10-15% (ç¾çŠ¶ã®èª¤åˆ†é¡20% â†’ 5-10%)
- **Latency**: Â±0 (å¤‰åŒ–ãªã—)
- **Cost**: Â±0 (å¤‰åŒ–ãªã—)

### å„ªå…ˆåº¦
â­â­â­â­â­ (æœ€å„ªå…ˆ) - accuracy å‘ä¸Šã¸ã®ç›´æ¥çš„ãªå½±éŸ¿

---

## ãƒãƒ¼ãƒ‰: generate_response

### åŸºæœ¬æƒ…å ±
- **ãƒ•ã‚¡ã‚¤ãƒ«**: src/nodes/generator.py:45-68
- **å½¹å‰²**: æœ€çµ‚çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘å¿œç­”ã‚’ç”Ÿæˆ
- **LLM ãƒ¢ãƒ‡ãƒ«**: claude-3-5-sonnet-20241022
- **ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: temperature=0.7, max_tokens=default

### ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
```python
ChatPromptTemplate.from_messages([
    ("system", "Generate helpful response based on context."),
    ("human", "{context}\n\nQuestion: {question}")
])
```

### å•é¡Œç‚¹
1. **å†—é•·æ€§åˆ¶å¾¡ãªã—**: ç°¡æ½”æ€§ã®æŒ‡ç¤ºãŒãªã„
2. **max_tokens æœªè¨­å®š**: ä¸å¿…è¦ã«é•·ã„å‡ºåŠ›ã®å¯èƒ½æ€§
3. **å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«æœªå®šç¾©**: ãƒˆãƒ¼ãƒ³ã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã®æŒ‡å®šãŒãªã„

### æ”¹å–„æ¡ˆ
1. "ç°¡æ½”ã«" "2-3æ–‡ã§" ãªã©ã®é•·ã•æŒ‡ç¤ºã‚’è¿½åŠ 
2. max_tokens ã‚’ 500 ã«åˆ¶é™
3. å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ˜ç¢ºåŒ–ï¼ˆ"è¦ªã—ã¿ã‚„ã™ã" "å°‚é–€çš„ã«" ãªã©ï¼‰

### æ¨å®šæ”¹å–„åŠ¹æœ
- **Accuracy**: Â±0 (å¤‰åŒ–ãªã—)
- **Latency**: -0.3-0.5s (å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ã«ã‚ˆã‚‹)
- **Cost**: -20-30% (ãƒˆãƒ¼ã‚¯ãƒ³æ•°å‰Šæ¸›ã«ã‚ˆã‚‹)

### å„ªå…ˆåº¦
â­â­â­ (ä¸­) - latency ã¨ cost ã®æ”¹å–„
```

## Phase 2: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡

### Step 4: è©•ä¾¡ç’°å¢ƒã®æº–å‚™

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹
- [ ] è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå®Ÿè¡Œå¯èƒ½
- [ ] ç’°å¢ƒå¤‰æ•°ï¼ˆAPI ã‚­ãƒ¼ãªã©ï¼‰ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹
- [ ] ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹

**å®Ÿè¡Œä¾‹**:
```bash
# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ç¢ºèª
cat tests/evaluation/test_cases.json

# è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å‹•ä½œç¢ºèª
uv run python -m src.evaluate --dry-run

# ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
echo $ANTHROPIC_API_KEY
```

### Step 5: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š

**æ¨å¥¨å®Ÿè¡Œå›æ•°**: 3-5 å›ï¼ˆçµ±è¨ˆçš„ãªä¿¡é ¼æ€§ã®ãŸã‚ï¼‰

**å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹**:
```bash
#!/bin/bash
# baseline_evaluation.sh

ITERATIONS=5
RESULTS_DIR="evaluation_results/baseline"
mkdir -p $RESULTS_DIR

for i in $(seq 1 $ITERATIONS); do
    echo "Running baseline evaluation: iteration $i/$ITERATIONS"
    uv run python -m src.evaluate \
        --output "$RESULTS_DIR/run_$i.json" \
        --verbose

    # API ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    sleep 5
done

# çµæœã®é›†è¨ˆ
uv run python -m src.aggregate_results \
    --input-dir "$RESULTS_DIR" \
    --output "$RESULTS_DIR/summary.json"
```

**è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹** (`src/evaluate.py`):
```python
import json
import time
from pathlib import Path
from typing import Dict, List

def evaluate_test_cases(test_cases: List[Dict]) -> Dict:
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è©•ä¾¡"""
    results = {
        "total_cases": len(test_cases),
        "correct": 0,
        "total_latency": 0.0,
        "total_cost": 0.0,
        "case_results": []
    }

    for case in test_cases:
        start_time = time.time()

        # LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        output = run_langgraph_app(case["input"])

        latency = time.time() - start_time

        # æ­£è§£åˆ¤å®š
        is_correct = output["answer"] == case["expected_answer"]
        if is_correct:
            results["correct"] += 1

        # ã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‹ã‚‰ï¼‰
        cost = calculate_cost(output["token_usage"])

        results["total_latency"] += latency
        results["total_cost"] += cost

        results["case_results"].append({
            "case_id": case["id"],
            "correct": is_correct,
            "latency": latency,
            "cost": cost
        })

    # æŒ‡æ¨™ã®è¨ˆç®—
    results["accuracy"] = (results["correct"] / results["total_cases"]) * 100
    results["avg_latency"] = results["total_latency"] / results["total_cases"]
    results["avg_cost"] = results["total_cost"] / results["total_cases"]

    return results

def calculate_cost(token_usage: Dict) -> float:
    """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‹ã‚‰ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—"""
    # Claude 3.5 Sonnet ã®æ–™é‡‘
    INPUT_COST_PER_1M = 3.0  # $3.00 per 1M input tokens
    OUTPUT_COST_PER_1M = 15.0  # $15.00 per 1M output tokens

    input_cost = (token_usage["input_tokens"] / 1_000_000) * INPUT_COST_PER_1M
    output_cost = (token_usage["output_tokens"] / 1_000_000) * OUTPUT_COST_PER_1M

    return input_cost + output_cost
```

### Step 6: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã®åˆ†æ

**é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹** (`src/aggregate_results.py`):
```python
import json
import numpy as np
from pathlib import Path
from typing import List, Dict

def aggregate_results(results_dir: Path) -> Dict:
    """è¤‡æ•°å›ã®å®Ÿè¡Œçµæœã‚’é›†è¨ˆ"""
    all_results = []

    for result_file in sorted(results_dir.glob("run_*.json")):
        with open(result_file) as f:
            all_results.append(json.load(f))

    # å„æŒ‡æ¨™ã®çµ±è¨ˆè¨ˆç®—
    accuracies = [r["accuracy"] for r in all_results]
    latencies = [r["avg_latency"] for r in all_results]
    costs = [r["avg_cost"] for r in all_results]

    summary = {
        "iterations": len(all_results),
        "accuracy": {
            "mean": np.mean(accuracies),
            "std": np.std(accuracies),
            "min": np.min(accuracies),
            "max": np.max(accuracies)
        },
        "latency": {
            "mean": np.mean(latencies),
            "std": np.std(latencies),
            "min": np.min(latencies),
            "max": np.max(latencies)
        },
        "cost": {
            "mean": np.mean(costs),
            "std": np.std(costs),
            "min": np.min(costs),
            "max": np.max(costs)
        }
    }

    return summary
```

**çµæœãƒ¬ãƒãƒ¼ãƒˆä¾‹**:
```markdown
# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è©•ä¾¡çµæœ

å®Ÿè¡Œæ—¥æ™‚: 2024-11-24 10:00:00
å®Ÿè¡Œå›æ•°: 5
ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: 20

## è©•ä¾¡æŒ‡æ¨™ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | å¹³å‡ | æ¨™æº–åå·® | æœ€å°å€¤ | æœ€å¤§å€¤ | ç›®æ¨™ | ã‚®ãƒ£ãƒƒãƒ— |
|------|------|----------|--------|--------|------|----------|
| Accuracy | 75.0% | 3.2% | 70.0% | 80.0% | 90.0% | **-15.0%** |
| Latency | 2.5s | 0.4s | 2.1s | 3.2s | 2.0s | **+0.5s** |
| Cost/req | $0.015 | $0.002 | $0.013 | $0.018 | $0.010 | **+$0.005** |

## è©³ç´°åˆ†æ

### Accuracy ã®å•é¡Œ
- **ç¾çŠ¶**: 75.0% (ç›®æ¨™: 90.0%)
- **ä¸»ãªèª¤ç­”ãƒ‘ã‚¿ãƒ¼ãƒ³**:
  1. ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡ãƒŸã‚¹: 12ã‚±ãƒ¼ã‚¹ (60%ã®èª¤ç­”)
  2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ä¸è¶³: 5ã‚±ãƒ¼ã‚¹ (25%ã®èª¤ç­”)
  3. æ›–æ˜§ãªè³ªå•ã¸ã®å¯¾å¿œ: 3ã‚±ãƒ¼ã‚¹ (15%ã®èª¤ç­”)

### Latency ã®å•é¡Œ
- **ç¾çŠ¶**: 2.5s (ç›®æ¨™: 2.0s)
- **ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**:
  1. generate_response ãƒãƒ¼ãƒ‰: å¹³å‡ 1.8s (å…¨ä½“ã®72%)
  2. analyze_intent ãƒãƒ¼ãƒ‰: å¹³å‡ 0.5s (å…¨ä½“ã®20%)
  3. ãã®ä»–: å¹³å‡ 0.2s (å…¨ä½“ã®8%)

### Cost ã®å•é¡Œ
- **ç¾çŠ¶**: $0.015/req (ç›®æ¨™: $0.010/req)
- **ã‚³ã‚¹ãƒˆå†…è¨³**:
  1. generate_response: $0.011 (73%)
  2. analyze_intent: $0.003 (20%)
  3. ãã®ä»–: $0.001 (7%)
- **ä¸»ãªè¦å› **: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šã„ï¼ˆå¹³å‡ 800 tokensï¼‰

## æ”¹å–„ã®æ–¹å‘æ€§

### å„ªå…ˆåº¦1: analyze_intent ã®ç²¾åº¦å‘ä¸Š
- **å½±éŸ¿**: Accuracy ã«ç›´æ¥å½±éŸ¿ï¼ˆ-15%ã®ã‚®ãƒ£ãƒƒãƒ—ã®60%ã‚’å ã‚ã‚‹ï¼‰
- **æ”¹å–„ç­–**: Few-shot examplesã€æ˜ç¢ºãªåˆ†é¡åŸºæº–ã€JSON å‡ºåŠ›å½¢å¼
- **æ¨å®šåŠ¹æœ**: +10-12% accuracy

### å„ªå…ˆåº¦2: generate_response ã®åŠ¹ç‡åŒ–
- **å½±éŸ¿**: Latency ã¨ Cost ã®ä¸¡æ–¹ã«å½±éŸ¿
- **æ”¹å–„ç­–**: ç°¡æ½”æ€§ã®æŒ‡ç¤ºã€max_tokens åˆ¶é™ã€temperature èª¿æ•´
- **æ¨å®šåŠ¹æœ**: -0.4s latency, -$0.004 cost
```

## Phase 3: åå¾©çš„æ”¹å–„

### Iteration ã®ã‚µã‚¤ã‚¯ãƒ«

å„ iteration ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œï¼š

1. **å„ªå…ˆé †ä½ä»˜ã‘** (Step 7)
2. **æ”¹å–„å®Ÿæ–½** (Step 8)
3. **æ”¹å–„å¾Œè©•ä¾¡** (Step 9)
4. **çµæœæ¯”è¼ƒ** (Step 10)
5. **ç¶™ç¶šåˆ¤æ–­** (Step 11)

### Step 7: å„ªå…ˆé †ä½ä»˜ã‘

**æ±ºå®šåŸºæº–**:
1. **ç›®æ¨™é”æˆã¸ã®å½±éŸ¿åº¦**
2. **æ”¹å–„ã®å®Ÿç¾å¯èƒ½æ€§**
3. **å®Ÿè£…ã‚³ã‚¹ãƒˆ**

**å„ªå…ˆé †ä½ãƒãƒˆãƒªãƒƒã‚¯ã‚¹**:
```markdown
## æ”¹å–„å„ªå…ˆé †ä½ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ãƒãƒ¼ãƒ‰ | å½±éŸ¿åº¦ | å®Ÿç¾å¯èƒ½æ€§ | å®Ÿè£…ã‚³ã‚¹ãƒˆ | ç·åˆã‚¹ã‚³ã‚¢ | å„ªå…ˆåº¦ |
|--------|--------|-----------|-----------|----------|--------|
| analyze_intent | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | 14/15 | 1ä½ |
| generate_response | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | 12/15 | 2ä½ |
| retrieve_context | â­â­ | â­â­â­ | â­â­â­ | 8/15 | 3ä½ |

**Iteration 1 ã®å¯¾è±¡**: analyze_intent ãƒãƒ¼ãƒ‰
```

### Step 8: æ”¹å–„å®Ÿæ–½

**æ”¹å–„å‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ** (`src/nodes/analyzer.py`):
```python
# Before
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=1.0
    )

    messages = [
        SystemMessage(content="You are an intent analyzer. Analyze user input."),
        HumanMessage(content=f"Analyze: {state['user_input']}")
    ]

    response = llm.invoke(messages)
    state["intent"] = response.content
    return state
```

**æ”¹å–„å¾Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**:
```python
# After - Iteration 1
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=0.3  # åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã¯ä½ã‚ã® temperature
    )

    # æ˜ç¢ºãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªã¨ few-shot examples
    system_prompt = """You are an intent classifier for a customer support chatbot.

Classify user input into one of these categories:
- "product_inquiry": Questions about products or services
- "technical_support": Technical issues or troubleshooting
- "billing": Payment, invoicing, or billing questions
- "general": General questions or chitchat

Output ONLY a valid JSON object with this structure:
{
  "intent": "<category>",
  "confidence": <0.0-1.0>,
  "reasoning": "<brief explanation>"
}

Examples:

Input: "How much does the premium plan cost?"
Output: {"intent": "product_inquiry", "confidence": 0.95, "reasoning": "Question about product pricing"}

Input: "I can't log into my account"
Output: {"intent": "technical_support", "confidence": 0.9, "reasoning": "Authentication issue"}

Input: "Why was I charged twice?"
Output: {"intent": "billing", "confidence": 0.95, "reasoning": "Question about billing charges"}

Input: "Hello, how are you?"
Output: {"intent": "general", "confidence": 0.85, "reasoning": "General greeting"}

Input: "What's the return policy?"
Output: {"intent": "product_inquiry", "confidence": 0.9, "reasoning": "Question about product policy"}
"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"Input: {state['user_input']}\nOutput:")
    ]

    response = llm.invoke(messages)

    # JSON ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
    try:
        intent_data = json.loads(response.content)
        state["intent"] = intent_data["intent"]
        state["confidence"] = intent_data["confidence"]
    except json.JSONDecodeError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        state["intent"] = "general"
        state["confidence"] = 0.5

    return state
```

**å¤‰æ›´å†…å®¹ã‚µãƒãƒªãƒ¼**:
1. âœ… temperature: 1.0 â†’ 0.3ï¼ˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸè¨­å®šï¼‰
2. âœ… æ˜ç¢ºãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªï¼ˆ4ã¤ã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼‰
3. âœ… Few-shot examplesï¼ˆ5å€‹è¿½åŠ ï¼‰
4. âœ… JSON å‡ºåŠ›å½¢å¼ï¼ˆæ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ï¼‰
5. âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆJSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

### Step 9: æ”¹å–„å¾Œè©•ä¾¡

**å®Ÿè¡Œ**:
```bash
# æ”¹å–„å¾Œã®è©•ä¾¡ã‚’åŒã˜æ¡ä»¶ã§å®Ÿè¡Œ
./evaluation_after_iteration1.sh
```

### Step 10: çµæœæ¯”è¼ƒ

**æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆä¾‹**:
```markdown
# Iteration 1 è©•ä¾¡çµæœ

å®Ÿè¡Œæ—¥æ™‚: 2024-11-24 12:00:00
å¤‰æ›´å†…å®¹: analyze_intent ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

## çµæœæ¯”è¼ƒ

| æŒ‡æ¨™ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | Iteration 1 | å¤‰åŒ– | å¤‰åŒ–ç‡ | ç›®æ¨™ | é”æˆç‡ |
|------|-------------|-------------|------|--------|------|--------|
| **Accuracy** | 75.0% | **86.0%** | **+11.0%** | +14.7% | 90.0% | 95.6% |
| **Latency** | 2.5s | 2.4s | -0.1s | -4.0% | 2.0s | 80.0% |
| **Cost/req** | $0.015 | $0.014 | -$0.001 | -6.7% | $0.010 | 71.4% |

## è©³ç´°åˆ†æ

### Accuracy ã®æ”¹å–„
- **å‘ä¸Š**: +11.0% (75.0% â†’ 86.0%)
- **æ®‹ã‚Šã‚®ãƒ£ãƒƒãƒ—**: 4.0% (ç›®æ¨™90.0%)
- **æ”¹å–„ã§ããŸã‚±ãƒ¼ã‚¹**: ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡ãƒŸã‚¹ãŒ 12 â†’ 3 ã‚±ãƒ¼ã‚¹ã«æ¸›å°‘
- **ã¾ã æ”¹å–„ãŒå¿…è¦**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ä¸è¶³ã®ã‚±ãƒ¼ã‚¹ï¼ˆ5ã‚±ãƒ¼ã‚¹ï¼‰

### Latency ã®è‹¥å¹²æ”¹å–„
- **å‘ä¸Š**: -0.1s (2.5s â†’ 2.4s)
- **ä¸»ãªè¦å› **: analyze_intent ã®æ¸©åº¦ä¸‹é™ã«ã‚ˆã‚Šå‡ºåŠ›ãŒç°¡æ½”ã«ãªã£ãŸ
- **æ®‹ã‚Šãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: generate_response (å¹³å‡ 1.8s)

### Cost ã®è‹¥å¹²å‰Šæ¸›
- **å‰Šæ¸›**: -$0.001 (6.7%å‰Šæ¸›)
- **è¦å› **: analyze_intent ã®å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
- **ä¸»ãªã‚³ã‚¹ãƒˆ**: generate_response ãŒä¾ç„¶ã¨ã—ã¦73%ã‚’å ã‚ã‚‹

## æ¬¡ã® Iteration ã®æ–¹é‡

### å„ªå…ˆåº¦1: generate_response ã®æœ€é©åŒ–
- **ç›®æ¨™**: Latency ã‚’ 1.8s â†’ 1.4sã€Cost ã‚’ $0.011 â†’ $0.007
- **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
  1. ç°¡æ½”æ€§ã®æŒ‡ç¤ºè¿½åŠ 
  2. max_tokens ã‚’ 500 ã«åˆ¶é™
  3. temperature ã‚’ 0.7 â†’ 0.5 ã«èª¿æ•´

### å„ªå…ˆåº¦2: Accuracy ã®æœ€å¾Œã®4%å‘ä¸Š
- **ç›®æ¨™**: 86.0% â†’ 90.0%ä»¥ä¸Š
- **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã‚’æ”¹å–„ï¼ˆretrieve_context ãƒãƒ¼ãƒ‰ï¼‰

## åˆ¤å®š
âœ… ç¶™ç¶š â†’ Iteration 2 ã«é€²ã‚€
```

### Step 11: ç¶™ç¶šåˆ¤æ–­

**åˆ¤æ–­åŸºæº–**:
```python
def should_continue_iteration(results: Dict, goals: Dict) -> bool:
    """Iteration ã‚’ç¶™ç¶šã™ã¹ãã‹åˆ¤æ–­"""
    all_goals_met = True

    for metric, goal in goals.items():
        if metric == "accuracy":
            if results[metric] < goal:
                all_goals_met = False
        elif metric in ["latency", "cost"]:
            if results[metric] > goal:
                all_goals_met = False

    return not all_goals_met

# ä¾‹
goals = {"accuracy": 90.0, "latency": 2.0, "cost": 0.010}
results = {"accuracy": 86.0, "latency": 2.4, "cost": 0.014}

if should_continue_iteration(results, goals):
    print("æ¬¡ã® Iteration ã«é€²ã‚€")
else:
    print("ç›®æ¨™é”æˆ - Phase 4 ã¸")
```

**Iteration ã®ä¸Šé™**:
- **æ¨å¥¨**: 3-5 iterations
- **ç†ç”±**: ãã‚Œä»¥ä¸Šã¯åç›Šé€“æ¸›ã®æ³•å‰‡ãŒåƒãå¯èƒ½æ€§ãŒé«˜ã„
- **ä¾‹å¤–**: Critical ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ 10+ iterations ã‚‚å¯

## Phase 4: å®Œäº†ã¨æ–‡æ›¸åŒ–

### Step 12: æœ€çµ‚è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ

**ãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```markdown
# LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå]
å®Ÿæ–½æœŸé–“: 2024-11-24 10:00 - 2024-11-24 15:00 (5æ™‚é–“)
å®Ÿæ–½è€…: Claude Code with fine-tune skill

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

æœ¬ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€LangGraph ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚’å®Ÿæ–½ã—ã€ä»¥ä¸‹ã®æˆæœã‚’é”æˆã—ã¾ã—ãŸï¼š

- âœ… **Accuracy**: 75.0% â†’ 92.0% (+17.0%, ç›®æ¨™90%é”æˆ)
- âœ… **Latency**: 2.5s â†’ 1.9s (-24.0%, ç›®æ¨™2.0sé”æˆ)
- âš ï¸ **Cost**: $0.015 â†’ $0.011 (-26.7%, ç›®æ¨™$0.010æœªé”)

å…¨3 iterations ã‚’å®Ÿæ–½ã—ã€3ã¤ã®æŒ‡æ¨™ã®ã†ã¡2ã¤ã§ç›®æ¨™ã‚’é”æˆã—ã¾ã—ãŸã€‚

## å®Ÿæ–½å†…å®¹ã‚µãƒãƒªãƒ¼

### Iteration æ•°ã¨å®Ÿè¡Œæ™‚é–“
- **Total Iterations**: 3
- **æœ€é©åŒ–ã—ãŸãƒãƒ¼ãƒ‰æ•°**: 2 (analyze_intent, generate_response)
- **è©•ä¾¡å®Ÿè¡Œå›æ•°**: 20å› (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³5å› + å„iterationå¾Œ5å›Ã—3)
- **ç·å®Ÿè¡Œæ™‚é–“**: ç´„5æ™‚é–“

### æœ€çµ‚çµæœ

| æŒ‡æ¨™ | åˆæœŸå€¤ | æœ€çµ‚å€¤ | æ”¹å–„ | æ”¹å–„ç‡ | ç›®æ¨™ | é”æˆçŠ¶æ³ |
|------|--------|--------|------|--------|------|---------|
| Accuracy | 75.0% | 92.0% | +17.0% | +22.7% | 90.0% | âœ… 102.2% é”æˆ |
| Latency | 2.5s | 1.9s | -0.6s | -24.0% | 2.0s | âœ… 95.0% é”æˆ |
| Cost/req | $0.015 | $0.011 | -$0.004 | -26.7% | $0.010 | âš ï¸ 90.9% é”æˆ |

## Iteration åˆ¥ã®è©³ç´°

### Iteration 1: analyze_intent ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 11:00
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/analyzer.py:25-45

**å¤‰æ›´å†…å®¹**:
1. temperature: 1.0 â†’ 0.3
2. Few-shot examples ã‚’5å€‹è¿½åŠ 
3. JSON å‡ºåŠ›å½¢å¼ã«æ§‹é€ åŒ–
4. æ˜ç¢ºãªåˆ†é¡ã‚«ãƒ†ã‚´ãƒªï¼ˆ4ã¤ï¼‰ã‚’å®šç¾©

**çµæœ**:
- Accuracy: 75.0% â†’ 86.0% (+11.0%)
- Latency: 2.5s â†’ 2.4s (-0.1s)
- Cost: $0.015 â†’ $0.014 (-$0.001)

**å­¦ã³**: Few-shot examples ã¨æ˜ç¢ºãªå‡ºåŠ›å½¢å¼ãŒ accuracy å‘ä¸Šã«æœ€ã‚‚åŠ¹æœçš„

---

### Iteration 2: generate_response ãƒãƒ¼ãƒ‰ã®æœ€é©åŒ–

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 13:00
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/generator.py:45-68

**å¤‰æ›´å†…å®¹**:
1. ç°¡æ½”æ€§ã®æŒ‡ç¤ºã‚’è¿½åŠ ï¼ˆ"2-3æ–‡ã§å›ç­”"ï¼‰
2. max_tokens: unlimited â†’ 500
3. temperature: 0.7 â†’ 0.5
4. å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ˜ç¢ºåŒ–

**çµæœ**:
- Accuracy: 86.0% â†’ 88.0% (+2.0%)
- Latency: 2.4s â†’ 2.0s (-0.4s)
- Cost: $0.014 â†’ $0.011 (-$0.003)

**å­¦ã³**: max_tokens åˆ¶é™ãŒ latency ã¨ cost å‰Šæ¸›ã«å¤§ããè²¢çŒ®

---

### Iteration 3: analyze_intent ã®è¿½åŠ æ”¹å–„

**å®Ÿæ–½æ—¥æ™‚**: 2024-11-24 14:30
**å¯¾è±¡ãƒãƒ¼ãƒ‰**: src/nodes/analyzer.py:25-45

**å¤‰æ›´å†…å®¹**:
1. Few-shot examples ã‚’ 5å€‹ â†’ 10å€‹ã«å¢—åŠ 
2. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ 
3. confidence threshold ã«ã‚ˆã‚‹å†åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯

**çµæœ**:
- Accuracy: 88.0% â†’ 92.0% (+4.0%)
- Latency: 2.0s â†’ 1.9s (-0.1s)
- Cost: $0.011 â†’ $0.011 (Â±0)

**å­¦ã³**: è¿½åŠ ã® few-shot examples ãŒ accuracy ã®æœ€å¾Œã®å£ã‚’çªç ´

## æœ€çµ‚çš„ãªå¤‰æ›´å†…å®¹

### src/nodes/analyzer.py (analyze_intent ãƒãƒ¼ãƒ‰)

#### Before
```python
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=1.0)
    messages = [
        SystemMessage(content="You are an intent analyzer. Analyze user input."),
        HumanMessage(content=f"Analyze: {state['user_input']}")
    ]
    response = llm.invoke(messages)
    state["intent"] = response.content
    return state
```

#### After
```python
def analyze_intent(state: GraphState) -> GraphState:
    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=0.3)

    system_prompt = """You are an intent classifier for a customer support chatbot.
Classify user input into: product_inquiry, technical_support, billing, or general.
Output JSON: {"intent": "<category>", "confidence": <0.0-1.0>, "reasoning": "<explanation>"}

[10 few-shot examples...]
"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"Input: {state['user_input']}\nOutput:")
    ]

    response = llm.invoke(messages)
    intent_data = json.loads(response.content)

    # Low confidence â†’ re-classify as general
    if intent_data["confidence"] < 0.7:
        intent_data["intent"] = "general"

    state["intent"] = intent_data["intent"]
    state["confidence"] = intent_data["confidence"]
    return state
```

**ä¸»ãªå¤‰æ›´ç‚¹**:
- temperature: 1.0 â†’ 0.3
- Few-shot examples: 0 â†’ 10
- å‡ºåŠ›: è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆ â†’ JSON
- Confidence threshold ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ 

---

### src/nodes/generator.py (generate_response ãƒãƒ¼ãƒ‰)

#### Before
```python
def generate_response(state: GraphState) -> GraphState:
    llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=0.7)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Generate helpful response based on context."),
        ("human", "{context}\n\nQuestion: {question}")
    ])
    chain = prompt | llm
    response = chain.invoke({"context": state["context"], "question": state["user_input"]})
    state["response"] = response.content
    return state
```

#### After
```python
def generate_response(state: GraphState) -> GraphState:
    llm = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        temperature=0.5,
        max_tokens=500  # å‡ºåŠ›é•·åˆ¶é™
    )

    system_prompt = """You are a helpful customer support assistant.

Guidelines:
- Be concise: Answer in 2-3 sentences
- Be friendly: Use a warm, professional tone
- Be accurate: Base your answer on the provided context
- If uncertain: Acknowledge and offer to escalate

Format: Direct answer followed by one optional clarifying sentence.
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "Context: {context}\n\nQuestion: {question}\n\nAnswer:")
    ])

    chain = prompt | llm
    response = chain.invoke({"context": state["context"], "question": state["user_input"]})
    state["response"] = response.content
    return state
```

**ä¸»ãªå¤‰æ›´ç‚¹**:
- temperature: 0.7 â†’ 0.5
- max_tokens: unlimited â†’ 500
- ç°¡æ½”æ€§ã®æ˜ç¢ºãªæŒ‡ç¤ºï¼ˆ"2-3 sentences"ï¼‰
- å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³è¿½åŠ 

## è©•ä¾¡çµæœã®è©³ç´°

### Test Case åˆ¥ã®æ”¹å–„çŠ¶æ³

| Case ID | Category | Before | After | æ”¹å–„ |
|---------|----------|--------|-------|------|
| TC001 | Product | âŒ Wrong | âœ… Correct | âœ… |
| TC002 | Technical | âŒ Wrong | âœ… Correct | âœ… |
| TC003 | Billing | âœ… Correct | âœ… Correct | - |
| TC004 | General | âœ… Correct | âœ… Correct | - |
| TC005 | Product | âŒ Wrong | âœ… Correct | âœ… |
| ... | ... | ... | ... | ... |
| TC020 | Technical | âœ… Correct | âœ… Correct | - |

**æ”¹å–„ã•ã‚ŒãŸã‚±ãƒ¼ã‚¹**: 15/20 (75%)
**ç¶­æŒã•ã‚ŒãŸã‚±ãƒ¼ã‚¹**: 5/20 (25%)
**åŠ£åŒ–ã—ãŸã‚±ãƒ¼ã‚¹**: 0/20 (0%)

### Latency ã®å†…è¨³

| ãƒãƒ¼ãƒ‰ | Before | After | å¤‰åŒ– | å¤‰åŒ–ç‡ |
|--------|--------|-------|------|--------|
| analyze_intent | 0.5s | 0.4s | -0.1s | -20% |
| retrieve_context | 0.2s | 0.2s | Â±0s | 0% |
| generate_response | 1.8s | 1.3s | -0.5s | -28% |
| **Total** | **2.5s** | **1.9s** | **-0.6s** | **-24%** |

### Cost ã®å†…è¨³

| ãƒãƒ¼ãƒ‰ | Before | After | å¤‰åŒ– | å¤‰åŒ–ç‡ |
|--------|--------|-------|------|--------|
| analyze_intent | $0.003 | $0.003 | Â±$0 | 0% |
| retrieve_context | $0.001 | $0.001 | Â±$0 | 0% |
| generate_response | $0.011 | $0.007 | -$0.004 | -36% |
| **Total** | **$0.015** | **$0.011** | **-$0.004** | **-27%** |

## ä»Šå¾Œã®æ¨å¥¨äº‹é …

### çŸ­æœŸï¼ˆ1-2é€±é–“ï¼‰
1. **Cost ç›®æ¨™ã®é”æˆ**: $0.011 â†’ $0.010
   - ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: Claude 3.5 Haiku ã¸ã®éƒ¨åˆ†ç§»è¡Œã‚’æ¤œè¨
   - æ¨å®šåŠ¹æœ: -$0.002-0.003/req

2. **Accuracy ã®æ›´ãªã‚‹å‘ä¸Š**: 92.0% â†’ 95.0%
   - ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã®åˆ†æã¨ few-shot examples ã®è¿½åŠ 
   - æ¨å®šåŠ¹æœ: +3.0%

### ä¸­æœŸï¼ˆ1-2ãƒ¶æœˆï¼‰
1. **ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–**
   - simple ãª intent classification ã«ã¯ Haiku ã‚’ä½¿ç”¨
   - complex ãª response generation ã®ã¿ Sonnet ã‚’ä½¿ç”¨
   - æ¨å®šåŠ¹æœ: -30-40% cost, latency ã¸ã®å½±éŸ¿ã¯æœ€å°

2. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ´»ç”¨**
   - System prompts ã¨ few-shot examples ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - æ¨å®šåŠ¹æœ: -50% costï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ï¼‰

### é•·æœŸï¼ˆ3-6ãƒ¶æœˆï¼‰
1. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨**
   - ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ã® model fine-tuning
   - Few-shot examples ä¸è¦ã§ç°¡æ½”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
   - æ¨å®šåŠ¹æœ: -60% cost, +5% accuracy

## çµè«–

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€LangGraph ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã‚’é”æˆã—ã¾ã—ãŸï¼š

âœ… **æˆåŠŸã—ãŸç‚¹**:
1. Accuracy ã®å¤§å¹…å‘ä¸Šï¼ˆ+22.7%ï¼‰- ç›®æ¨™ã‚’2.2%è¶…éé”æˆ
2. Latency ã®é¡•è‘—ãªæ”¹å–„ï¼ˆ-24.0%ï¼‰- ç›®æ¨™ã‚’5%è¶…éé”æˆ
3. Cost ã®å‰Šæ¸›ï¼ˆ-26.7%ï¼‰- ç›®æ¨™ã«ã‚ã¨9.1%

âš ï¸ **èª²é¡Œ**:
1. Cost ç›®æ¨™æœªé”ï¼ˆ$0.011 vs $0.010ç›®æ¨™ï¼‰- è»½é‡ãƒ¢ãƒ‡ãƒ«ã¸ã®ç§»è¡Œã§å¯¾å¿œå¯èƒ½

ğŸ“ˆ **ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã®å‘ä¸Šï¼ˆaccuracyå‘ä¸Šã«ã‚ˆã‚Šï¼‰
- é‹ç”¨ã‚³ã‚¹ãƒˆã®å‰Šæ¸›ï¼ˆlatency, costå‰Šæ¸›ã«ã‚ˆã‚Šï¼‰
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ”¹å–„ï¼ˆåŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ï¼‰

ğŸ¯ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**:
1. Cost å‰Šæ¸›ã®ãŸã‚ã®è»½é‡ãƒ¢ãƒ‡ãƒ«ç§»è¡Œã®æ¤œè¨¼
2. ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨è©•ä¾¡
3. æ–°ã—ã„ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¸ã®å±•é–‹

---

ä½œæˆæ—¥æ™‚: 2024-11-24 15:00:00
ä½œæˆè€…: Claude Code (fine-tune skill)
```

### Step 13: ã‚³ãƒ¼ãƒ‰ã‚³ãƒŸãƒƒãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°

**Git ã‚³ãƒŸãƒƒãƒˆä¾‹**:
```bash
# å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ
git add src/nodes/analyzer.py src/nodes/generator.py
git commit -m "feat: optimize LangGraph prompts for accuracy and latency

Iteration 1-3 of fine-tuning process:
- analyze_intent: added few-shot examples, JSON output, lower temperature
- generate_response: added conciseness guidelines, max_tokens limit

Results:
- Accuracy: 75.0% â†’ 92.0% (+17.0%, goal 90% âœ…)
- Latency: 2.5s â†’ 1.9s (-0.6s, goal 2.0s âœ…)
- Cost: $0.015 â†’ $0.011 (-$0.004, goal $0.010 âš ï¸)

Full report: evaluation_results/final_report.md"

# è©•ä¾¡çµæœã‚‚ã‚³ãƒŸãƒƒãƒˆ
git add evaluation_results/
git commit -m "docs: add fine-tuning evaluation results and final report"

# ã‚¿ã‚°ã‚’ä»˜ã‘ã‚‹
git tag -a fine-tune-v1.0 -m "Fine-tuning completed: 92% accuracy achieved"
```

## ã¾ã¨ã‚

ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å¾“ã†ã“ã¨ã§ï¼š
- âœ… ä½“ç³»çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ
- âœ… ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®æ„æ€æ±ºå®š
- âœ… ç¶™ç¶šçš„ãªæ”¹å–„ã¨æ¤œè¨¼
- âœ… å®Œå…¨ãªæ–‡æ›¸åŒ–ã¨ãƒˆãƒ¬ãƒ¼ã‚µãƒ“ãƒªãƒ†ã‚£

ãŒå®Ÿç¾ã§ãã¾ã™ã€‚
